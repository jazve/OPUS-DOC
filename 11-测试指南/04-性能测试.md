# æ€§èƒ½æµ‹è¯•æŒ‡å—

## ğŸ¯ æ€§èƒ½æµ‹è¯•æ¦‚è¿°

æ€§èƒ½æµ‹è¯•ç¡®ä¿OPUSç³»ç»Ÿåœ¨å„ç§è´Ÿè½½æ¡ä»¶ä¸‹çš„å“åº”æ—¶é—´ã€ååé‡å’Œèµ„æºä½¿ç”¨æ»¡è¶³ä¸šåŠ¡éœ€æ±‚ã€‚

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æŒ‡æ ‡ (KPI)
- **å“åº”æ—¶é—´** - æ™ºèƒ½ä½“ç”Ÿæˆ < 30ç§’
- **ååé‡** - å¹¶å‘ç”¨æˆ· â‰¥ 100
- **èµ„æºä½¿ç”¨** - å†…å­˜ < 512MBï¼ŒCPU < 80%
- **å¯ç”¨æ€§** - 99.9% æ­£å¸¸è¿è¡Œæ—¶é—´

### å…·ä½“æ€§èƒ½ç›®æ ‡
```yaml
æ€§èƒ½åŸºå‡†:
  æ™ºèƒ½ä½“ç”Ÿæˆ:
    å¹³å‡å“åº”æ—¶é—´: < 15ç§’
    P95å“åº”æ—¶é—´: < 30ç§’
    P99å“åº”æ—¶é—´: < 60ç§’
  
  è¯­æ³•è§£æ:
    å¹³å‡å“åº”æ—¶é—´: < 50ms
    P95å“åº”æ—¶é—´: < 100ms
    P99å“åº”æ—¶é—´: < 200ms
  
  å¹¶å‘å¤„ç†:
    æœ€å¤§å¹¶å‘ç”¨æˆ·: 100
    å¹³å‡ååé‡: 10 req/sec
    å³°å€¼ååé‡: 50 req/sec
```

## ğŸ› ï¸ æ€§èƒ½æµ‹è¯•å·¥å…·

### Pythonæ€§èƒ½æµ‹è¯•æ¡†æ¶
```python
# requirements-performance.txt
pytest-benchmark>=4.0.0
locust>=2.14.0
memory-profiler>=0.60.0
line-profiler>=4.0.0
pytest-monitor>=1.6.0
psutil>=5.9.0
```

### æµ‹è¯•å·¥å…·é…ç½®
```python
# pytest-benchmarké…ç½®
pytest.ini
[tool:pytest]
addopts = --benchmark-only --benchmark-sort=mean
markers =
    performance: marks tests as performance tests
    benchmark: marks tests for benchmarking
    stress: marks tests as stress tests
```

## ğŸ“‹ æ€§èƒ½æµ‹è¯•ç±»å‹

### åŸºå‡†æµ‹è¯• (Benchmark)
```python
import pytest
from opus.core import AgentGenerator

class TestPerformanceBenchmarks:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    @pytest.mark.benchmark
    def test_agent_generation_benchmark(self, benchmark):
        """æ™ºèƒ½ä½“ç”Ÿæˆæ€§èƒ½åŸºå‡†"""
        generator = AgentGenerator()
        request = {
            "description": "Pythonä»£ç å®¡æŸ¥åŠ©æ‰‹",
            "requirements": ["ä»£ç åˆ†æ", "å»ºè®®ç”Ÿæˆ"]
        }
        
        # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
        result = benchmark(generator.generate, request)
        
        # éªŒè¯ç»“æœ
        assert result.success
        assert result.agent.is_valid()
    
    @pytest.mark.benchmark
    def test_opus_parsing_benchmark(self, benchmark):
        """OPUSè¯­æ³•è§£ææ€§èƒ½åŸºå‡†"""
        from opus.parser import OpusParser
        
        opus_content = """
        # Identity
        name: å¤æ‚åŠ©æ‰‹
        role: é«˜çº§AIåŠ©æ‰‹
        expertise: [AI, ML, NLP, Python, JavaScript]
        
        # Architecture
        knowledge_domains: [æŠ€æœ¯, ç§‘å­¦, å•†ä¸š]
        skills: [åˆ†æ, ç”Ÿæˆ, æ¨ç†, åˆ›ä½œ]
        
        # Memory
        type: long_term
        retention: 90d
        structure: hierarchical
        
        # Workflow
        steps:
          - input_analysis
          - context_retrieval
          - reasoning
          - response_generation
          - quality_check
        """
        
        parser = OpusParser()
        result = benchmark(parser.parse, opus_content)
        
        assert result.is_valid()
```

### è´Ÿè½½æµ‹è¯• (Load Testing)
```python
from locust import HttpUser, task, between

class OpusLoadTest(HttpUser):
    """OPUSç³»ç»Ÿè´Ÿè½½æµ‹è¯•"""
    
    wait_time = between(1, 5)  # ç”¨æˆ·è¯·æ±‚é—´éš”
    
    def on_start(self):
        """æµ‹è¯•å¼€å§‹æ—¶çš„åˆå§‹åŒ–"""
        self.client.verify = False  # å¿½ç•¥SSLéªŒè¯
    
    @task(3)
    def generate_simple_agent(self):
        """ç”Ÿæˆç®€å•æ™ºèƒ½ä½“ï¼ˆé«˜é¢‘ä»»åŠ¡ï¼‰"""
        response = self.client.post("/api/generate", json={
            "description": "ç®€å•çš„é—®ç­”åŠ©æ‰‹",
            "type": "basic"
        })
        
        if response.status_code == 200:
            assert "agent" in response.json()
    
    @task(2)
    def generate_complex_agent(self):
        """ç”Ÿæˆå¤æ‚æ™ºèƒ½ä½“ï¼ˆä¸­é¢‘ä»»åŠ¡ï¼‰"""
        response = self.client.post("/api/generate", json={
            "description": "ä¸“ä¸šçš„ä»£ç å®¡æŸ¥åŠ©æ‰‹",
            "requirements": [
                "å¤šè¯­è¨€æ”¯æŒ",
                "æ€§èƒ½åˆ†æ",
                "å®‰å…¨æ£€æŸ¥"
            ],
            "type": "advanced"
        })
        
        if response.status_code == 200:
            result = response.json()
            assert result["agent"]["complexity"] == "advanced"
    
    @task(1)
    def analyze_requirements(self):
        """éœ€æ±‚åˆ†æï¼ˆä½é¢‘ä»»åŠ¡ï¼‰"""
        response = self.client.post("/api/analyze", json={
            "description": "æˆ‘éœ€è¦ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å®¢æˆ·æœåŠ¡çš„AIåŠ©æ‰‹"
        })
        
        assert response.status_code == 200
```

### å‹åŠ›æµ‹è¯• (Stress Testing)
```python
import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor

class StressTest:
    """ç³»ç»Ÿå‹åŠ›æµ‹è¯•"""
    
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
    
    async def single_request(self, session, request_data):
        """å•ä¸ªè¯·æ±‚"""
        try:
            start_time = time.time()
            async with session.post(
                f"{self.base_url}/api/generate",
                json=request_data
            ) as response:
                result = await response.json()
                end_time = time.time()
                
                return {
                    "success": response.status == 200,
                    "response_time": end_time - start_time,
                    "result": result
                }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response_time": None
            }
    
    async def stress_test(self, concurrent_users=50, duration=300):
        """æ‰§è¡Œå‹åŠ›æµ‹è¯•"""
        print(f"å¼€å§‹å‹åŠ›æµ‹è¯•: {concurrent_users}å¹¶å‘ç”¨æˆ·, {duration}ç§’")
        
        start_time = time.time()
        results = []
        
        async with aiohttp.ClientSession() as session:
            while time.time() - start_time < duration:
                # åˆ›å»ºå¹¶å‘è¯·æ±‚
                tasks = []
                for i in range(concurrent_users):
                    request_data = {
                        "description": f"å‹åŠ›æµ‹è¯•åŠ©æ‰‹{i}",
                        "type": "basic"
                    }
                    tasks.append(self.single_request(session, request_data))
                
                # æ‰§è¡Œå¹¶å‘è¯·æ±‚
                batch_results = await asyncio.gather(*tasks)
                results.extend(batch_results)
                
                # çŸ­æš‚ä¼‘æ¯
                await asyncio.sleep(1)
        
        return self.analyze_results(results)
    
    def analyze_results(self, results):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        successful = [r for r in results if r["success"]]
        failed = [r for r in results if not r["success"]]
        
        response_times = [r["response_time"] for r in successful if r["response_time"]]
        
        return {
            "total_requests": len(results),
            "successful_requests": len(successful),
            "failed_requests": len(failed),
            "success_rate": len(successful) / len(results) * 100,
            "avg_response_time": sum(response_times) / len(response_times) if response_times else 0,
            "max_response_time": max(response_times) if response_times else 0,
            "min_response_time": min(response_times) if response_times else 0
        }

# è¿è¡Œå‹åŠ›æµ‹è¯•
async def run_stress_test():
    stress_test = StressTest()
    results = await stress_test.stress_test(concurrent_users=20, duration=60)
    print(f"å‹åŠ›æµ‹è¯•ç»“æœ: {results}")

# pytestä½¿ç”¨
@pytest.mark.stress
@pytest.mark.asyncio
async def test_system_stress():
    """ç³»ç»Ÿå‹åŠ›æµ‹è¯•"""
    stress_test = StressTest()
    results = await stress_test.stress_test(concurrent_users=10, duration=30)
    
    assert results["success_rate"] >= 95  # æˆåŠŸç‡è¦æ±‚
    assert results["avg_response_time"] <= 30  # å¹³å‡å“åº”æ—¶é—´è¦æ±‚
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### èµ„æºä½¿ç”¨ç›‘æ§
```python
import psutil
import time
import threading
from dataclasses import dataclass

@dataclass
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡"""
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    disk_io_read_mb: float
    disk_io_write_mb: float
    network_sent_mb: float
    network_recv_mb: float
    timestamp: float

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = []
        self.monitoring = False
        self.monitor_thread = None
    
    def start_monitoring(self, interval=1):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            args=(interval,)
        )
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
    
    def _monitor_loop(self, interval):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            metrics = self._collect_metrics()
            self.metrics.append(metrics)
            time.sleep(interval)
    
    def _collect_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        return SystemMetrics(
            cpu_percent=psutil.cpu_percent(),
            memory_percent=psutil.virtual_memory().percent,
            memory_used_mb=psutil.virtual_memory().used / 1024 / 1024,
            disk_io_read_mb=psutil.disk_io_counters().read_bytes / 1024 / 1024,
            disk_io_write_mb=psutil.disk_io_counters().write_bytes / 1024 / 1024,
            network_sent_mb=psutil.net_io_counters().bytes_sent / 1024 / 1024,
            network_recv_mb=psutil.net_io_counters().bytes_recv / 1024 / 1024,
            timestamp=time.time()
        )
    
    def get_summary(self):
        """è·å–ç›‘æ§æ‘˜è¦"""
        if not self.metrics:
            return None
        
        cpu_values = [m.cpu_percent for m in self.metrics]
        memory_values = [m.memory_percent for m in self.metrics]
        
        return {
            "duration": self.metrics[-1].timestamp - self.metrics[0].timestamp,
            "avg_cpu_percent": sum(cpu_values) / len(cpu_values),
            "max_cpu_percent": max(cpu_values),
            "avg_memory_percent": sum(memory_values) / len(memory_values),
            "max_memory_percent": max(memory_values),
            "peak_memory_mb": max(m.memory_used_mb for m in self.metrics)
        }

# ä½¿ç”¨ç¤ºä¾‹
@pytest.mark.performance
def test_agent_generation_with_monitoring():
    """å¸¦ç›‘æ§çš„æ™ºèƒ½ä½“ç”Ÿæˆæµ‹è¯•"""
    monitor = PerformanceMonitor()
    
    try:
        # å¼€å§‹ç›‘æ§
        monitor.start_monitoring(interval=0.5)
        
        # æ‰§è¡Œæµ‹è¯•
        generator = AgentGenerator()
        for i in range(10):
            result = generator.generate({
                "description": f"æµ‹è¯•åŠ©æ‰‹{i}",
                "type": "basic"
            })
            assert result.success
        
        # åœæ­¢ç›‘æ§
        monitor.stop_monitoring()
        
        # åˆ†æç»“æœ
        summary = monitor.get_summary()
        print(f"æ€§èƒ½æ‘˜è¦: {summary}")
        
        # æ–­è¨€æ€§èƒ½è¦æ±‚
        assert summary["max_cpu_percent"] < 80  # CPUä½¿ç”¨ç‡ä¸è¶…è¿‡80%
        assert summary["peak_memory_mb"] < 512  # å†…å­˜ä½¿ç”¨ä¸è¶…è¿‡512MB
        
    finally:
        monitor.stop_monitoring()
```

## ğŸ” æ€§èƒ½åˆ†æ

### ä»£ç æ€§èƒ½åˆ†æ
```python
# ä½¿ç”¨line_profilerè¿›è¡Œè¡Œçº§æ€§èƒ½åˆ†æ
@profile
def generate_agent_profiled(description):
    """å¸¦æ€§èƒ½åˆ†æçš„æ™ºèƒ½ä½“ç”Ÿæˆ"""
    generator = AgentGenerator()
    
    # æ­¥éª¤1: éœ€æ±‚åˆ†æ
    analysis = generator.analyze_requirements(description)
    
    # æ­¥éª¤2: é…ç½®ç”Ÿæˆ
    config = generator.generate_config(analysis)
    
    # æ­¥éª¤3: æ™ºèƒ½ä½“åˆ›å»º
    agent = generator.create_agent(config)
    
    return agent

# ä½¿ç”¨memory_profilerè¿›è¡Œå†…å­˜åˆ†æ
@profile
def memory_intensive_operation():
    """å†…å­˜å¯†é›†å‹æ“ä½œåˆ†æ"""
    large_data = []
    for i in range(10000):
        large_data.append({
            "id": i,
            "data": "x" * 1000,
            "metadata": {"created": time.time()}
        })
    
    # å¤„ç†æ•°æ®
    processed = [item for item in large_data if item["id"] % 2 == 0]
    
    return processed
```

### æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
```python
import cProfile
import pstats
from contextlib import contextmanager

@contextmanager
def profile_code():
    """ä»£ç æ€§èƒ½åˆ†æä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    profiler = cProfile.Profile()
    profiler.enable()
    
    try:
        yield profiler
    finally:
        profiler.disable()

def analyze_performance_bottlenecks():
    """åˆ†ææ€§èƒ½ç“¶é¢ˆ"""
    with profile_code() as profiler:
        # æ‰§è¡Œè¢«åˆ†æçš„ä»£ç 
        generator = AgentGenerator()
        for i in range(5):
            generator.generate({
                "description": f"æ€§èƒ½æµ‹è¯•åŠ©æ‰‹{i}",
                "complexity": "high"
            })
    
    # åˆ†æç»“æœ
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    
    # è¾“å‡ºtop 10è€—æ—¶å‡½æ•°
    print("Top 10 most time-consuming functions:")
    stats.print_stats(10)
    
    return stats
```

## ğŸ“Š æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

### è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ
```python
import json
import matplotlib.pyplot as plt
from datetime import datetime

class PerformanceReporter:
    """æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.results = []
    
    def add_test_result(self, test_name, metrics):
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        self.results.append({
            "test_name": test_name,
            "timestamp": datetime.now().isoformat(),
            "metrics": metrics
        })
    
    def generate_report(self, output_path="performance_report.html"):
        """ç”ŸæˆHTMLæ€§èƒ½æŠ¥å‘Š"""
        html_content = self._create_html_report()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
    
    def _create_html_report(self):
        """åˆ›å»ºHTMLæŠ¥å‘Šå†…å®¹"""
        return f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>OPUSæ€§èƒ½æµ‹è¯•æŠ¥å‘Š</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .metric {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; }}
                .success {{ background-color: #d4edda; }}
                .warning {{ background-color: #fff3cd; }}
                .danger {{ background-color: #f8d7da; }}
            </style>
        </head>
        <body>
            <h1>OPUSæ€§èƒ½æµ‹è¯•æŠ¥å‘Š</h1>
            <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            
            <h2>æµ‹è¯•æ‘˜è¦</h2>
            {self._generate_summary_html()}
            
            <h2>è¯¦ç»†ç»“æœ</h2>
            {self._generate_details_html()}
        </body>
        </html>
        """
    
    def _generate_summary_html(self):
        """ç”Ÿæˆæ‘˜è¦HTML"""
        if not self.results:
            return "<p>æš‚æ— æµ‹è¯•ç»“æœ</p>"
        
        total_tests = len(self.results)
        avg_response_time = sum(
            r["metrics"].get("avg_response_time", 0) 
            for r in self.results
        ) / total_tests
        
        return f"""
        <div class="metric success">
            <h3>æ€»ä½“æŒ‡æ ‡</h3>
            <p>æµ‹è¯•æ•°é‡: {total_tests}</p>
            <p>å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ç§’</p>
        </div>
        """
    
    def _generate_details_html(self):
        """ç”Ÿæˆè¯¦ç»†ç»“æœHTML"""
        details_html = ""
        
        for result in self.results:
            css_class = self._get_result_css_class(result["metrics"])
            details_html += f"""
            <div class="metric {css_class}">
                <h3>{result['test_name']}</h3>
                <p>æ‰§è¡Œæ—¶é—´: {result['timestamp']}</p>
                {self._format_metrics_html(result['metrics'])}
            </div>
            """
        
        return details_html
    
    def _get_result_css_class(self, metrics):
        """æ ¹æ®æŒ‡æ ‡ç¡®å®šCSSç±»"""
        response_time = metrics.get("avg_response_time", 0)
        success_rate = metrics.get("success_rate", 100)
        
        if response_time > 30 or success_rate < 95:
            return "danger"
        elif response_time > 15 or success_rate < 98:
            return "warning"
        else:
            return "success"
    
    def _format_metrics_html(self, metrics):
        """æ ¼å¼åŒ–æŒ‡æ ‡ä¸ºHTML"""
        html = "<ul>"
        for key, value in metrics.items():
            html += f"<li>{key}: {value}</li>"
        html += "</ul>"
        return html

# ä½¿ç”¨ç¤ºä¾‹
def test_with_reporting():
    """å¸¦æŠ¥å‘Šçš„æ€§èƒ½æµ‹è¯•"""
    reporter = PerformanceReporter()
    
    # æ‰§è¡Œå¤šä¸ªæ€§èƒ½æµ‹è¯•
    tests = [
        ("æ™ºèƒ½ä½“ç”Ÿæˆ", {"avg_response_time": 12.5, "success_rate": 99.2}),
        ("è¯­æ³•è§£æ", {"avg_response_time": 0.08, "success_rate": 100.0}),
        ("å¹¶å‘å¤„ç†", {"avg_response_time": 18.3, "success_rate": 97.8})
    ]
    
    for test_name, metrics in tests:
        reporter.add_test_result(test_name, metrics)
    
    # ç”ŸæˆæŠ¥å‘Š
    reporter.generate_report()
```

## ğŸš¦ æŒç»­æ€§èƒ½ç›‘æ§

### CI/CDæ€§èƒ½ç›‘æ§é›†æˆ
```yaml
# .github/workflows/performance.yml
name: Performance Tests

on:
  push:
    branches: [main]
  schedule:
    - cron: '0 2 * * *'  # æ¯å¤©å‡Œæ™¨2ç‚¹è¿è¡Œ

jobs:
  performance:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements-performance.txt
    
    - name: Run performance tests
      run: |
        pytest tests/performance/ --benchmark-json=benchmark.json
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: benchmark.json
    
    - name: Performance regression check
      run: |
        python scripts/check_performance_regression.py benchmark.json
```

## ğŸ“ æœ€ä½³å®è·µ

### æ€§èƒ½æµ‹è¯•ç­–ç•¥
1. **æ—©æœŸé›†æˆ** - åœ¨å¼€å‘è¿‡ç¨‹ä¸­æŒç»­è¿›è¡Œæ€§èƒ½æµ‹è¯•
2. **çœŸå®æ•°æ®** - ä½¿ç”¨æ¥è¿‘ç”Ÿäº§ç¯å¢ƒçš„æ•°æ®é‡
3. **å¤šç»´åº¦æµ‹è¯•** - å“åº”æ—¶é—´ã€ååé‡ã€èµ„æºä½¿ç”¨
4. **å›å½’æ£€æµ‹** - ç›‘æ§æ€§èƒ½é€€åŒ–

### å¸¸è§è¯¯åŒº
- âŒ **åªæµ‹è¯•æœ€ä½³æƒ…å†µ** - å¿½ç•¥è¾¹ç•Œå’Œå¼‚å¸¸æƒ…å†µ
- âŒ **æµ‹è¯•ç¯å¢ƒä¸çœŸå®** - ä¸ç”Ÿäº§ç¯å¢ƒå·®å¼‚è¿‡å¤§
- âŒ **å¿½ç•¥èµ„æºç›‘æ§** - åªå…³æ³¨å“åº”æ—¶é—´
- âŒ **ç¼ºä¹åŸºå‡†å¯¹æ¯”** - æ²¡æœ‰å†å²æ€§èƒ½åŸºå‡†

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [æµ‹è¯•æ¦‚è¿°](01-æµ‹è¯•æ¦‚è¿°.md) - æ•´ä½“æµ‹è¯•ç­–ç•¥
- [å•å…ƒæµ‹è¯•æŒ‡å—](02-å•å…ƒæµ‹è¯•.md) - å•å…ƒæµ‹è¯•æ–¹æ³•
- [é›†æˆæµ‹è¯•æŒ‡å—](03-é›†æˆæµ‹è¯•.md) - é›†æˆæµ‹è¯•ç­–ç•¥
- [è‡ªåŠ¨åŒ–æµ‹è¯•](06-è‡ªåŠ¨åŒ–æµ‹è¯•.md) - CI/CDé›†æˆ

---

*ğŸ¯ æ€§èƒ½æµ‹è¯•ç¡®ä¿ç³»ç»Ÿåœ¨å„ç§è´Ÿè½½ä¸‹ç¨³å®šè¿è¡Œï¼Œæ˜¯äº§å“ä¸Šçº¿çš„é‡è¦ä¿éšœã€‚*