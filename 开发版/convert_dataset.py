#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SSAPæ¡†æ¶LoRAæ•°æ®é›†è½¬æ¢è„šæœ¬
å°†Markdownæ ¼å¼çš„è®­ç»ƒæ•°æ®è½¬æ¢ä¸ºæ ‡å‡†JSONæ ¼å¼
"""

import json
import re
import random
from typing import List, Dict, Any
from pathlib import Path

class SSAPDatasetConverter:
    """SSAPæ•°æ®é›†è½¬æ¢å™¨"""
    
    def __init__(self, input_file: str = "SSAP_LoRA_Dataset.md"):
        self.input_file = input_file
        self.output_file = "SSAP_LoRA_Dataset.json"
        self.samples = []
        
        # æ•°æ®ç±»åˆ«é…ç½®
        self.categories = {
            "A": {"name": "åŸºç¡€æ¶æ„", "count": 250, "samples": []},
            "B": {"name": "å·¥ä½œæµç¼–æ’", "count": 300, "samples": []},
            "C": {"name": "æ ¼å¼è¾“å‡º", "count": 200, "samples": []},
            "D": {"name": "æ™ºèƒ½ååŒ", "count": 150, "samples": []}
        }
        
        # æŒ‡ä»¤æ¨¡æ¿åº“
        self.instruction_templates = {
            "A": [
                "åŸºäºSSAPæ¡†æ¶è®¾è®¡ä¸€ä¸ª{domain}æ™ºèƒ½ä½“",
                "åˆ›å»ºä¸€ä¸ªä¸“ç²¾{domain}çš„SSAPæ¶æ„æ™ºèƒ½ä½“",
                "æ„å»ºå…·å¤‡{features}èƒ½åŠ›çš„{domain}ä¸“å®¶ç³»ç»Ÿ",
                "è®¾è®¡ä¸€ä¸ªä½¿ç”¨SSAPæ¡†æ¶çš„{domain}ä¸“ä¸šæ™ºèƒ½ä½“",
                "åŸºäºSSAPä¸ƒå±‚æ¶æ„å¼€å‘{domain}é¢†åŸŸä¸“å®¶"
            ],
            "B": [
                "è®¾è®¡ä¸€ä¸ª{workflow_type}çš„SSAPå·¥ä½œæµ",
                "åˆ›å»ºåŒ…å«{features}çš„å·¥ä½œæµç¼–æ’",
                "æ„å»º{complexity}çš„SSAPä¼ªä»£ç å·¥ä½œæµ",
                "è®¾è®¡å…·å¤‡{capabilities}çš„æ™ºèƒ½å·¥ä½œæµ",
                "åŸºäºSSAPæ¡†æ¶ç¼–æ’{scenario}å·¥ä½œæµç¨‹"
            ],
            "C": [
                "ç”Ÿæˆ{format_type}çš„SSAPæ ¼å¼åŒ–è¾“å‡º",
                "åˆ›å»º{content_type}çš„ä¸“ä¸šæ ¼å¼å±•ç¤º",
                "è®¾è®¡{scenario}çš„æ ¼å¼æ¨¡å—åº”ç”¨",
                "æ„å»º{output_style}çš„å†…å®¹å‘ˆç°æ ¼å¼",
                "åŸºäºSSAPæ ¼å¼æ¨¡å—ç”Ÿæˆ{document_type}"
            ],
            "D": [
                "è®¾è®¡{collaboration_type}çš„æ™ºèƒ½ååŒæ–¹æ¡ˆ",
                "åˆ›å»º{complexity}çš„SSAPæ™ºèƒ½ååŒæµç¨‹",
                "æ„å»ºå…·å¤‡{features}çš„ååŒæ™ºèƒ½ç³»ç»Ÿ",
                "è®¾è®¡{scenario}çš„æ€ç»´é“¾ååŒæœºåˆ¶",
                "åŸºäºSSAPæ¡†æ¶å®ç°{capability}ååŒ"
            ]
        }
    
    def parse_markdown_samples(self) -> None:
        """è§£æMarkdownæ–‡ä»¶ä¸­çš„è®­ç»ƒæ ·æœ¬"""
        try:
            with open(self.input_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–ä¸åŒç±»åˆ«çš„æ ·æœ¬
            for category in self.categories.keys():
                pattern = rf"### {category}-(\d+)"
                samples = re.findall(
                    rf"{pattern}.*?```\n(.*?)\n```.*?\*\*æ ‡æ³¨\*\*:\n- \*\*è‹±æ–‡\*\*: (.*?)\n- \*\*ä¸­æ–‡\*\*: (.*?)\n",
                    content,
                    re.DOTALL
                )
                
                for sample_id, training_text, english_labels, chinese_labels in samples:
                    sample = {
                        "category": category,
                        "id": f"{category}-{sample_id}",
                        "training_text": training_text.strip(),
                        "english_labels": english_labels.strip(),
                        "chinese_labels": chinese_labels.strip()
                    }
                    self.categories[category]["samples"].append(sample)
            
            print(f"âœ… æˆåŠŸè§£æ {sum(len(cat['samples']) for cat in self.categories.values())} ä¸ªæ ·æœ¬")
            
        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {self.input_file}")
            raise
        except Exception as e:
            print(f"âŒ è§£ææ–‡ä»¶æ—¶å‡ºé”™: {e}")
            raise
    
    def extract_sample_features(self, sample: Dict[str, str]) -> Dict[str, str]:
        """ä»æ ·æœ¬ä¸­æå–ç‰¹å¾ä¿¡æ¯"""
        training_text = sample["training_text"]
        
        # æå–èº«ä»½ä¿¡æ¯
        identity_match = re.search(r'ä½ æ˜¯"([^"]+)"', training_text)
        identity = identity_match.group(1) if identity_match else "ä¸“ä¸šæ™ºèƒ½ä½“"
        
        # æå–ä¸“ä¸šé¢†åŸŸ
        domain_patterns = [
            r'ä¸“ç²¾([^ï¼Œå’Œ]+)',
            r'ä¸“ä¸šçŸ¥è¯†ï¼š([^+\n]+)',
            r'æ ¸å¿ƒæŠ€èƒ½ï¼š([^+\n]+)'
        ]
        
        domain = identity
        for pattern in domain_patterns:
            match = re.search(pattern, training_text)
            if match:
                domain = match.group(1).strip()
                break
        
        # æå–å…³é”®ç‰¹å¾
        features = []
        if "[Thinking." in training_text:
            features.append("æ€ç»´é“¾")
        if "[Context." in training_text:
            features.append("ä¸Šä¸‹æ–‡ç®¡ç†")
        if "[Memory." in training_text:
            features.append("è®°å¿†ç®¡ç†")
        if "FUNCTION" in training_text:
            features.append("å·¥ä½œæµç¼–æ’")
        if "æ ¼å¼æ¨¡å—" in training_text:
            features.append("æ ¼å¼æ§åˆ¶")
        
        return {
            "identity": identity,
            "domain": domain,
            "features": "+".join(features) if features else "åŸºç¡€åŠŸèƒ½"
        }
    
    def generate_instruction(self, category: str, features: Dict[str, str]) -> str:
        """ç”ŸæˆæŒ‡ä»¤æ–‡æœ¬"""
        templates = self.instruction_templates.get(category, [])
        if not templates:
            return f"åŸºäºSSAPæ¡†æ¶è®¾è®¡ä¸€ä¸ª{features['domain']}æ™ºèƒ½ä½“"
        
        template = random.choice(templates)
        
        # æ ¹æ®ç±»åˆ«å¡«å……ä¸åŒçš„å ä½ç¬¦
        if category == "A":
            return template.format(
                domain=features["domain"],
                features=features["features"]
            )
        elif category == "B":
            workflow_types = ["æ¡ä»¶åˆ†æ”¯", "å¾ªç¯å¤„ç†", "å¼‚å¸¸å¤„ç†", "è®°å¿†é©±åŠ¨", "æ™ºèƒ½ååŒ"]
            return template.format(
                workflow_type=random.choice(workflow_types),
                features=features["features"],
                complexity="å¤æ‚çš„" if "ååŒ" in features["features"] else "æ ‡å‡†çš„",
                capabilities="è‡ªé€‚åº”" if "æ€ç»´é“¾" in features["features"] else "åŸºç¡€",
                scenario="ä¸šåŠ¡å¤„ç†"
            )
        elif category == "C":
            format_types = ["åˆ†ææŠ¥å‘Š", "æ–¹æ¡ˆè®¾è®¡", "äº¤äº’ç•Œé¢", "å®æ—¶ç›‘æ§"]
            return template.format(
                format_type=random.choice(format_types),
                content_type="ä¸“ä¸š",
                scenario="ä¸šåŠ¡åº”ç”¨",
                output_style="ç»“æ„åŒ–",
                document_type="æŠ€æœ¯æ–‡æ¡£"
            )
        elif category == "D":
            collaboration_types = ["æ€ç»´é“¾ååŒ", "ä¸Šä¸‹æ–‡æ„ŸçŸ¥", "å¤šæ¨¡æ€ååŒ", "è®°å¿†é©±åŠ¨"]
            return template.format(
                collaboration_type=random.choice(collaboration_types),
                complexity="é«˜å¤æ‚åº¦" if len(features["features"]) > 10 else "ä¸­ç­‰å¤æ‚åº¦",
                features=features["features"],
                scenario="å¤æ‚é—®é¢˜è§£å†³",
                capability="æ™ºèƒ½å†³ç­–"
            )
        
        return template
    
    def convert_to_training_format(self) -> List[Dict[str, Any]]:
        """è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼"""
        training_data = []
        
        for category, info in self.categories.items():
            print(f"ğŸ”„ å¤„ç† {category}ç±» - {info['name']} æ•°æ®...")
            
            for sample in info["samples"]:
                # æå–ç‰¹å¾
                features = self.extract_sample_features(sample)
                
                # ç”ŸæˆæŒ‡ä»¤
                instruction = self.generate_instruction(category, features)
                
                # åˆ›å»ºè®­ç»ƒæ ·æœ¬
                training_sample = {
                    "instruction": instruction,
                    "input": "",  # SSAPæ¡†æ¶é€šå¸¸ä¸éœ€è¦é¢å¤–è¾“å…¥
                    "output": sample["training_text"],
                    "labels": {
                        "english": sample["english_labels"],
                        "chinese": sample["chinese_labels"]
                    },
                    "metadata": {
                        "category": category,
                        "category_name": info["name"],
                        "sample_id": sample["id"],
                        "domain": features["domain"],
                        "features": features["features"],
                        "complexity": self._assess_complexity(sample["training_text"])
                    }
                }
                
                training_data.append(training_sample)
        
        print(f"âœ… ç”Ÿæˆ {len(training_data)} ä¸ªè®­ç»ƒæ ·æœ¬")
        return training_data
    
    def _assess_complexity(self, text: str) -> str:
        """è¯„ä¼°æ ·æœ¬å¤æ‚åº¦"""
        complexity_score = 0
        
        # æ¶æ„å¤æ‚åº¦
        if "æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†" in text:
            complexity_score += 3
        if "åŠ¨æ€æ€ç»´é“¾" in text:
            complexity_score += 3
        if "è®°å¿†ç®¡ç†" in text:
            complexity_score += 2
        
        # é€»è¾‘å¤æ‚åº¦
        if text.count("IF") > 2:
            complexity_score += 2
        if "WHILE" in text:
            complexity_score += 2
        if text.count("FUNCTION") > 1:
            complexity_score += 1
        
        # åŠŸèƒ½å¤æ‚åº¦
        if text.count("[") > 5:  # å¤šæ¨¡å—è°ƒç”¨
            complexity_score += 2
        if "ååŒ" in text:
            complexity_score += 2
        
        if complexity_score >= 8:
            return "é«˜"
        elif complexity_score >= 4:
            return "ä¸­"
        else:
            return "ä½"
    
    def expand_dataset(self, base_samples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰©å±•æ•°æ®é›†åˆ°ç›®æ ‡è§„æ¨¡"""
        expanded_samples = base_samples.copy()
        
        for category, info in self.categories.items():
            current_count = len([s for s in base_samples if s["metadata"]["category"] == category])
            target_count = info["count"]
            
            if current_count < target_count:
                needed = target_count - current_count
                category_samples = [s for s in base_samples if s["metadata"]["category"] == category]
                
                print(f"ğŸ“ˆ æ‰©å±• {category}ç±»æ•°æ®: {current_count} -> {target_count} (+{needed})")
                
                # é€šè¿‡å˜ä½“ç”Ÿæˆæ‰©å±•æ ·æœ¬
                for _ in range(needed):
                    base_sample = random.choice(category_samples)
                    variant = self._create_variant(base_sample, category)
                    expanded_samples.append(variant)
        
        print(f"âœ… æ•°æ®é›†æ‰©å±•å®Œæˆï¼Œæ€»è®¡ {len(expanded_samples)} ä¸ªæ ·æœ¬")
        return expanded_samples
    
    def _create_variant(self, base_sample: Dict[str, Any], category: str) -> Dict[str, Any]:
        """åˆ›å»ºæ ·æœ¬å˜ä½“"""
        variant = base_sample.copy()
        
        # ä¿®æ”¹æŒ‡ä»¤
        features = {
            "domain": variant["metadata"]["domain"],
            "features": variant["metadata"]["features"]
        }
        variant["instruction"] = self.generate_instruction(category, features)
        
        # æ›´æ–°å…ƒæ•°æ®
        variant["metadata"] = variant["metadata"].copy()
        variant["metadata"]["sample_id"] = f"{category}-variant-{random.randint(1000, 9999)}"
        variant["metadata"]["is_variant"] = True
        
        return variant
    
    def split_train_eval(self, samples: List[Dict[str, Any]], 
                        train_ratio: float = 0.8) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®"""
        random.shuffle(samples)
        
        split_idx = int(len(samples) * train_ratio)
        train_samples = samples[:split_idx]
        eval_samples = samples[split_idx:]
        
        print(f"ğŸ“Š æ•°æ®åˆ†å‰²: è®­ç»ƒé›† {len(train_samples)} ä¸ªï¼ŒéªŒè¯é›† {len(eval_samples)} ä¸ª")
        return train_samples, eval_samples
    
    def save_dataset(self, train_samples: List[Dict[str, Any]], 
                    eval_samples: List[Dict[str, Any]]) -> None:
        """ä¿å­˜æ•°æ®é›†"""
        dataset = {
            "train": train_samples,
            "eval": eval_samples,
            "metadata": {
                "total_samples": len(train_samples) + len(eval_samples),
                "train_samples": len(train_samples),
                "eval_samples": len(eval_samples),
                "categories": {
                    cat: {
                        "name": info["name"],
                        "target_count": info["count"],
                        "actual_count": len([s for s in train_samples + eval_samples 
                                           if s["metadata"]["category"] == cat])
                    }
                    for cat, info in self.categories.items()
                },
                "complexity_distribution": self._get_complexity_distribution(train_samples + eval_samples),
                "domain_distribution": self._get_domain_distribution(train_samples + eval_samples)
            }
        }
        
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ•°æ®é›†å·²ä¿å­˜åˆ°: {self.output_file}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        self._save_statistics(dataset["metadata"])
    
    def _get_complexity_distribution(self, samples: List[Dict[str, Any]]) -> Dict[str, int]:
        """è·å–å¤æ‚åº¦åˆ†å¸ƒ"""
        distribution = {"é«˜": 0, "ä¸­": 0, "ä½": 0}
        for sample in samples:
            complexity = sample["metadata"]["complexity"]
            distribution[complexity] += 1
        return distribution
    
    def _get_domain_distribution(self, samples: List[Dict[str, Any]]) -> Dict[str, int]:
        """è·å–é¢†åŸŸåˆ†å¸ƒ"""
        distribution = {}
        for sample in samples:
            domain = sample["metadata"]["domain"]
            distribution[domain] = distribution.get(domain, 0) + 1
        return dict(sorted(distribution.items(), key=lambda x: x[1], reverse=True))
    
    def _save_statistics(self, metadata: Dict[str, Any]) -> None:
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats_content = f"""# SSAPæ¡†æ¶LoRAæ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š

## ğŸ“Š æ€»ä½“ç»Ÿè®¡
- **æ€»æ ·æœ¬æ•°**: {metadata['total_samples']}
- **è®­ç»ƒé›†**: {metadata['train_samples']} ä¸ª ({metadata['train_samples']/metadata['total_samples']*100:.1f}%)
- **éªŒè¯é›†**: {metadata['eval_samples']} ä¸ª ({metadata['eval_samples']/metadata['total_samples']*100:.1f}%)

## ğŸ“‹ ç±»åˆ«åˆ†å¸ƒ
"""
        
        for cat, info in metadata['categories'].items():
            percentage = info['actual_count'] / metadata['total_samples'] * 100
            stats_content += f"- **{cat}ç±» - {info['name']}**: {info['actual_count']} ä¸ª ({percentage:.1f}%)\n"
        
        stats_content += f"""
## ğŸ¯ å¤æ‚åº¦åˆ†å¸ƒ
"""
        for complexity, count in metadata['complexity_distribution'].items():
            percentage = count / metadata['total_samples'] * 100
            stats_content += f"- **{complexity}å¤æ‚åº¦**: {count} ä¸ª ({percentage:.1f}%)\n"
        
        stats_content += f"""
## ğŸ¢ é¢†åŸŸåˆ†å¸ƒ (Top 10)
"""
        for i, (domain, count) in enumerate(list(metadata['domain_distribution'].items())[:10], 1):
            percentage = count / metadata['total_samples'] * 100
            stats_content += f"{i}. **{domain}**: {count} ä¸ª ({percentage:.1f}%)\n"
        
        with open("dataset_statistics.md", 'w', encoding='utf-8') as f:
            f.write(stats_content)
        
        print("ğŸ“ˆ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: dataset_statistics.md")
    
    def convert(self) -> None:
        """æ‰§è¡Œå®Œæ•´è½¬æ¢æµç¨‹"""
        print("ğŸš€ å¼€å§‹SSAPæ•°æ®é›†è½¬æ¢...")
        
        # 1. è§£æMarkdownæ ·æœ¬
        self.parse_markdown_samples()
        
        # 2. è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
        base_samples = self.convert_to_training_format()
        
        # 3. æ‰©å±•æ•°æ®é›†
        all_samples = self.expand_dataset(base_samples)
        
        # 4. åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
        train_samples, eval_samples = self.split_train_eval(all_samples)
        
        # 5. ä¿å­˜æ•°æ®é›†
        self.save_dataset(train_samples, eval_samples)
        
        print("âœ… SSAPæ•°æ®é›†è½¬æ¢å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {self.output_file}")
        print("ğŸ”§ ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†è¿›è¡ŒLoRAè®­ç»ƒäº†")

def main():
    """ä¸»å‡½æ•°"""
    converter = SSAPDatasetConverter()
    converter.convert()

if __name__ == "__main__":
    main() 