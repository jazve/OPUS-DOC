#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ OpenRouteré›†æˆæµ‹è¯•è¿è¡Œå™¨
ä½¿ç”¨OpenRouter APIè¿›è¡ŒçœŸå®çš„AIæ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°
"""

import sys
import os
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ æ ¸å¿ƒæ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "core"))

from core.prompt_testing_framework import PromptTestingFramework
from core.evaluation_metrics import ComprehensiveEvaluator
from core.openrouter_integration import OpenRouterClient, MultiModelTester, AIEvaluator
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('openrouter_tests.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class OpenRouterTestRunner:
    """OpenRouteré›†æˆæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, config_path: str = "config/openrouter_config.json"):
        try:
            self.client = OpenRouterClient(config_path)
            self.multi_model_tester = MultiModelTester(self.client)
            self.ai_evaluator = AIEvaluator(self.client)
            self.framework = PromptTestingFramework()
            self.evaluator = ComprehensiveEvaluator()
            
            # åŠ è½½æµ‹è¯•ç”¨ä¾‹
            self.framework.load_test_cases_from_json("examples/example_test_cases.json")
            
            logger.info("OpenRouteræµ‹è¯•è¿è¡Œå™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def create_ai_response_function(self, model: str):
        """åˆ›å»ºAIå“åº”å‡½æ•°"""
        def ai_response_func(prompt: str) -> str:
            try:
                response = self.client.generate_response(model, prompt)
                return response
            except Exception as e:
                logger.error(f"AIå“åº”ç”Ÿæˆå¤±è´¥: {e}")
                return f"é”™è¯¯: æ— æ³•ç”Ÿæˆå“åº” - {str(e)}"
        
        return ai_response_func
    
    def run_single_model_test(self, model: str, test_suite: str = "basic") -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæ¨¡å‹æµ‹è¯•"""
        logger.info(f"å¼€å§‹æµ‹è¯•æ¨¡å‹: {model}")
        
        # é€‰æ‹©æµ‹è¯•ç”¨ä¾‹
        if test_suite == "basic":
            test_ids = ["ssap_role_001", "ssap_format_001", "workflow_001", "performance_001"]
        elif test_suite == "comprehensive":
            test_ids = [tc.id for tc in self.framework.test_cases]
        elif test_suite == "ssap":
            test_ids = [tc.id for tc in self.framework.test_cases if "ssap" in tc.id]
        elif test_suite == "workflow":
            test_ids = [tc.id for tc in self.framework.test_cases if "workflow" in tc.id]
        else:
            test_ids = [tc.id for tc in self.framework.test_cases]
        
        selected_tests = [tc for tc in self.framework.test_cases if tc.id in test_ids]
        
        if not selected_tests:
            logger.error(f"æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹: {test_suite}")
            return {'error': 'No test cases found'}
        
        logger.info(f"è¿è¡Œ {len(selected_tests)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        # åˆ›å»ºAIå“åº”å‡½æ•°
        ai_response_func = self.create_ai_response_function(model)
        
        # è¿è¡Œæµ‹è¯•
        results = []
        for test_case in selected_tests:
            logger.info(f"æ‰§è¡Œæµ‹è¯•: {test_case.name}")
            
            start_time = datetime.now()
            result = self.framework.run_single_test(test_case, ai_response_func)
            end_time = datetime.now()
            
            # å¢å¼ºç»“æœæ•°æ®
            enhanced_result = {
                'test_case': {
                    'id': test_case.id,
                    'name': test_case.name,
                    'category': test_case.category,
                    'priority': test_case.priority,
                    'input_text': test_case.input_text,
                    'expected_patterns': test_case.expected_patterns,
                    'expected_format': test_case.expected_format
                },
                'result': {
                    'passed': result.passed,
                    'score': result.score,
                    'response_time': result.response_time,
                    'output_text': result.output_text,
                    'pattern_matches': result.pattern_matches,
                    'format_compliance': result.format_compliance,
                    'error_messages': result.error_messages,
                    'timestamp': result.timestamp
                },
                'model': model,
                'execution_time': (end_time - start_time).total_seconds()
            }
            
            # å¦‚æœå¯ç”¨äº†AIè¯„ä¼°
            if self.client.config['evaluation_settings']['enable_ai_evaluation']:
                try:
                    ai_evaluation = self.ai_evaluator.evaluate_response(
                        test_case.input_text,
                        result.output_text,
                        'content_quality'
                    )
                    enhanced_result['ai_evaluation'] = ai_evaluation
                except Exception as e:
                    logger.error(f"AIè¯„ä¼°å¤±è´¥: {e}")
                    enhanced_result['ai_evaluation'] = {'error': str(e)}
            
            results.append(enhanced_result)
            
            # æ˜¾ç¤ºè¿›åº¦
            status = "âœ… é€šè¿‡" if result.passed else "âŒ å¤±è´¥"
            score_display = f"{result.score:.3f}"
            time_display = f"{result.response_time:.2f}s"
            
            print(f"  {status} {test_case.name} - åˆ†æ•°: {score_display}, æ—¶é—´: {time_display}")
        
        # è®¡ç®—æ‘˜è¦ç»Ÿè®¡
        passed_tests = sum(1 for r in results if r['result']['passed'])
        total_tests = len(results)
        avg_score = sum(r['result']['score'] for r in results) / total_tests if total_tests > 0 else 0
        avg_time = sum(r['result']['response_time'] for r in results) / total_tests if total_tests > 0 else 0
        
        summary = {
            'model': model,
            'test_suite': test_suite,
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': total_tests - passed_tests,
            'success_rate': passed_tests / total_tests if total_tests > 0 else 0,
            'average_score': avg_score,
            'average_response_time': avg_time,
            'timestamp': datetime.now().isoformat(),
            'results': results
        }
        
        # è·å–æˆæœ¬ä¿¡æ¯
        cost_summary = self.client.get_cost_summary()
        summary['cost_summary'] = cost_summary
        
        return summary
    
    def run_model_comparison(self, models: List[str], test_suite: str = "basic") -> Dict[str, Any]:
        """è¿è¡Œæ¨¡å‹æ¯”è¾ƒæµ‹è¯•"""
        logger.info(f"å¼€å§‹æ¨¡å‹æ¯”è¾ƒæµ‹è¯•: {models}")
        
        comparison_results = {
            'models': models,
            'test_suite': test_suite,
            'timestamp': datetime.now().isoformat(),
            'individual_results': {},
            'comparison_analysis': {}
        }
        
        # åˆ†åˆ«æµ‹è¯•æ¯ä¸ªæ¨¡å‹
        for model in models:
            try:
                model_result = self.run_single_model_test(model, test_suite)
                comparison_results['individual_results'][model] = model_result
            except Exception as e:
                logger.error(f"æ¨¡å‹ {model} æµ‹è¯•å¤±è´¥: {e}")
                comparison_results['individual_results'][model] = {
                    'error': str(e),
                    'model': model
                }
        
        # ç”Ÿæˆæ¯”è¾ƒåˆ†æ
        if len(comparison_results['individual_results']) > 1:
            comparison_analysis = self._generate_comparison_analysis(comparison_results['individual_results'])
            comparison_results['comparison_analysis'] = comparison_analysis
        
        return comparison_results
    
    def run_performance_benchmark(self, models: List[str], iterations: int = 3) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info(f"å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•: {models}, è¿­ä»£æ¬¡æ•°: {iterations}")
        
        # é€‰æ‹©æ€§èƒ½æµ‹è¯•ç”¨ä¾‹
        performance_test_cases = [
            tc for tc in self.framework.test_cases 
            if tc.category in ['performance', 'basic', 'workflow_generation']
        ][:5]  # å–å‰5ä¸ªæµ‹è¯•ç”¨ä¾‹
        
        benchmark_results = {
            'models': models,
            'iterations': iterations,
            'timestamp': datetime.now().isoformat(),
            'detailed_results': [],
            'summary': {}
        }
        
        for iteration in range(iterations):
            logger.info(f"åŸºå‡†æµ‹è¯•è¿­ä»£ {iteration + 1}/{iterations}")
            
            iteration_results = {}
            
            for model in models:
                logger.info(f"æµ‹è¯•æ¨¡å‹: {model}")
                
                model_results = []
                total_time = 0
                
                for test_case in performance_test_cases:
                    start_time = datetime.now()
                    
                    # ç›´æ¥ä½¿ç”¨OpenRouter API
                    try:
                        response = self.client.generate_response(model, test_case.input_text)
                        end_time = datetime.now()
                        
                        response_time = (end_time - start_time).total_seconds()
                        total_time += response_time
                        
                        model_results.append({
                            'test_case_id': test_case.id,
                            'response_time': response_time,
                            'response_length': len(response),
                            'success': True
                        })
                        
                    except Exception as e:
                        logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
                        model_results.append({
                            'test_case_id': test_case.id,
                            'response_time': 0,
                            'response_length': 0,
                            'success': False,
                            'error': str(e)
                        })
                
                iteration_results[model] = {
                    'total_time': total_time,
                    'avg_response_time': total_time / len(performance_test_cases),
                    'successful_tests': sum(1 for r in model_results if r['success']),
                    'total_tests': len(performance_test_cases),
                    'test_results': model_results
                }
            
            benchmark_results['detailed_results'].append({
                'iteration': iteration + 1,
                'results': iteration_results
            })
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        for model in models:
            model_stats = {
                'avg_response_time': 0,
                'min_response_time': float('inf'),
                'max_response_time': 0,
                'success_rate': 0,
                'consistency_score': 0
            }
            
            response_times = []
            success_counts = []
            
            for iteration_result in benchmark_results['detailed_results']:
                if model in iteration_result['results']:
                    model_data = iteration_result['results'][model]
                    response_times.append(model_data['avg_response_time'])
                    success_counts.append(model_data['successful_tests'])
                    
                    model_stats['min_response_time'] = min(model_stats['min_response_time'], model_data['avg_response_time'])
                    model_stats['max_response_time'] = max(model_stats['max_response_time'], model_data['avg_response_time'])
            
            if response_times:
                model_stats['avg_response_time'] = sum(response_times) / len(response_times)
                model_stats['success_rate'] = sum(success_counts) / (len(success_counts) * len(performance_test_cases))
                
                # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•° (åŸºäºå“åº”æ—¶é—´çš„æ ‡å‡†å·®)
                if len(response_times) > 1:
                    import statistics
                    std_dev = statistics.stdev(response_times)
                    mean_time = statistics.mean(response_times)
                    model_stats['consistency_score'] = max(0, 1 - (std_dev / mean_time))
                else:
                    model_stats['consistency_score'] = 1.0
            
            benchmark_results['summary'][model] = model_stats
        
        return benchmark_results
    
    def _generate_comparison_analysis(self, individual_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¯”è¾ƒåˆ†æ"""
        analysis = {
            'performance_ranking': [],
            'cost_analysis': {},
            'quality_comparison': {},
            'recommendations': []
        }
        
        # æ€§èƒ½æ’å
        model_performance = []
        for model, result in individual_results.items():
            if 'error' not in result:
                model_performance.append({
                    'model': model,
                    'success_rate': result.get('success_rate', 0),
                    'average_score': result.get('average_score', 0),
                    'average_response_time': result.get('average_response_time', 0)
                })
        
        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        model_performance.sort(key=lambda x: x['success_rate'] * 0.4 + x['average_score'] * 0.6, reverse=True)
        analysis['performance_ranking'] = model_performance
        
        # æˆæœ¬åˆ†æ
        cost_data = {}
        for model, result in individual_results.items():
            if 'error' not in result and 'cost_summary' in result:
                cost_info = result['cost_summary']
                cost_data[model] = {
                    'total_cost': cost_info.get('total_cost', 0),
                    'avg_cost_per_request': cost_info.get('average_cost_per_request', 0),
                    'requests_count': cost_info.get('requests_count', 0)
                }
        
        analysis['cost_analysis'] = cost_data
        
        # è´¨é‡æ¯”è¾ƒ
        quality_data = {}
        for model, result in individual_results.items():
            if 'error' not in result:
                quality_data[model] = {
                    'average_score': result.get('average_score', 0),
                    'success_rate': result.get('success_rate', 0),
                    'response_time': result.get('average_response_time', 0)
                }
        
        analysis['quality_comparison'] = quality_data
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        if model_performance:
            best_model = model_performance[0]['model']
            recommendations.append(f"ç»¼åˆæ€§èƒ½æœ€ä½³æ¨¡å‹: {best_model}")
            
            # æˆæœ¬æ•ˆç›Šåˆ†æ
            if cost_data:
                cost_efficient = min(cost_data.keys(), key=lambda x: cost_data[x]['avg_cost_per_request'])
                recommendations.append(f"æˆæœ¬æ•ˆç›Šæœ€ä½³æ¨¡å‹: {cost_efficient}")
                
                # é€Ÿåº¦æœ€å¿«
                fastest_model = min(quality_data.keys(), key=lambda x: quality_data[x]['response_time'])
                recommendations.append(f"å“åº”é€Ÿåº¦æœ€å¿«æ¨¡å‹: {fastest_model}")
        
        analysis['recommendations'] = recommendations
        
        return analysis
    
    def save_results(self, results: Dict[str, Any], filename: str):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        os.makedirs("reports", exist_ok=True)
        
        filepath = f"reports/{filename}"
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def print_summary(self, results: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ¯ OpenRouteræµ‹è¯•ç»“æœæ‘˜è¦")
        print("="*60)
        
        if 'individual_results' in results:
            # æ¨¡å‹æ¯”è¾ƒç»“æœ
            print("\nğŸ“Š æ¨¡å‹æ¯”è¾ƒç»“æœ:")
            for model, result in results['individual_results'].items():
                if 'error' not in result:
                    print(f"  {model}:")
                    print(f"    æˆåŠŸç‡: {result.get('success_rate', 0):.1%}")
                    print(f"    å¹³å‡åˆ†æ•°: {result.get('average_score', 0):.3f}")
                    print(f"    å¹³å‡å“åº”æ—¶é—´: {result.get('average_response_time', 0):.2f}s")
                    
                    if 'cost_summary' in result:
                        cost = result['cost_summary'].get('total_cost', 0)
                        print(f"    æ€»æˆæœ¬: ${cost:.6f}")
                else:
                    print(f"  {model}: âŒ æµ‹è¯•å¤±è´¥ - {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            # æ¯”è¾ƒåˆ†æ
            if 'comparison_analysis' in results:
                analysis = results['comparison_analysis']
                
                print("\nğŸ† æ€§èƒ½æ’å:")
                for i, model_perf in enumerate(analysis['performance_ranking'], 1):
                    print(f"  {i}. {model_perf['model']} - ç»¼åˆå¾—åˆ†: {model_perf['success_rate'] * 0.4 + model_perf['average_score'] * 0.6:.3f}")
                
                print("\nğŸ’¡ å»ºè®®:")
                for rec in analysis['recommendations']:
                    print(f"  â€¢ {rec}")
        
        elif 'summary' in results:
            # åŸºå‡†æµ‹è¯•ç»“æœ
            print("\nâš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ:")
            for model, stats in results['summary'].items():
                print(f"  {model}:")
                print(f"    å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.2f}s")
                print(f"    æˆåŠŸç‡: {stats['success_rate']:.1%}")
                print(f"    ä¸€è‡´æ€§åˆ†æ•°: {stats['consistency_score']:.3f}")
        
        else:
            # å•ä¸ªæ¨¡å‹ç»“æœ
            print(f"\nğŸ“ˆ æ¨¡å‹: {results.get('model', 'Unknown')}")
            print(f"  æµ‹è¯•å¥—ä»¶: {results.get('test_suite', 'Unknown')}")
            print(f"  æ€»æµ‹è¯•æ•°: {results.get('total_tests', 0)}")
            print(f"  é€šè¿‡æµ‹è¯•: {results.get('passed_tests', 0)}")
            print(f"  æˆåŠŸç‡: {results.get('success_rate', 0):.1%}")
            print(f"  å¹³å‡åˆ†æ•°: {results.get('average_score', 0):.3f}")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {results.get('average_response_time', 0):.2f}s")
            
            if 'cost_summary' in results:
                cost = results['cost_summary'].get('total_cost', 0)
                print(f"  æ€»æˆæœ¬: ${cost:.6f}")
        
        print("\nğŸ’° æˆæœ¬æ‘˜è¦:")
        if 'cost_summary' in results:
            cost_summary = results['cost_summary']
            print(f"  æ€»æˆæœ¬: ${cost_summary.get('total_cost', 0):.6f}")
            print(f"  è¯·æ±‚æ•°: {cost_summary.get('requests_count', 0)}")
            print(f"  å¹³å‡æ¯è¯·æ±‚æˆæœ¬: ${cost_summary.get('average_cost_per_request', 0):.6f}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="OpenRouteré›†æˆæµ‹è¯•è¿è¡Œå™¨")
    parser.add_argument("--mode", 
                       choices=["single", "compare", "benchmark", "list-models"], 
                       default="single",
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--model", 
                       default="deepseek/deepseek-r1-0528:free",
                       help="è¦æµ‹è¯•çš„æ¨¡å‹")
    parser.add_argument("--models", 
                       nargs="+",
                       help="è¦æ¯”è¾ƒçš„æ¨¡å‹åˆ—è¡¨")
    parser.add_argument("--test-suite", 
                       choices=["basic", "comprehensive", "ssap", "workflow"],
                       default="basic",
                       help="æµ‹è¯•å¥—ä»¶")
    parser.add_argument("--iterations", 
                       type=int, 
                       default=3,
                       help="åŸºå‡†æµ‹è¯•è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--output", 
                       help="è¾“å‡ºæ–‡ä»¶å")
    parser.add_argument("--verbose", 
                       action="store_true",
                       help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        runner = OpenRouterTestRunner()
        
        print("ğŸš€ OpenRouter AIæ¨¡å‹æµ‹è¯•ç³»ç»Ÿ")
        print("="*60)
        
        if args.mode == "list-models":
            print("ğŸ“‹ å¯ç”¨æ¨¡å‹:")
            models = runner.client.list_available_models()
            for model in models:
                print(f"  â€¢ {model.id} - {model.name}")
                print(f"    æè¿°: {model.description}")
                print(f"    æˆæœ¬: ${model.cost_per_token:.8f}/token")
                print(f"    æ¨èç”¨é€”: {', '.join(model.recommended_for)}")
                print()
        
        elif args.mode == "single":
            print(f"ğŸ¯ å•æ¨¡å‹æµ‹è¯•: {args.model}")
            print(f"æµ‹è¯•å¥—ä»¶: {args.test_suite}")
            
            results = runner.run_single_model_test(args.model, args.test_suite)
            runner.print_summary(results)
            
            if args.output:
                runner.save_results(results, args.output)
        
        elif args.mode == "compare":
            if not args.models:
                args.models = [
                    "deepseek/deepseek-r1-0528:free",
                    "deepseek/deepseek-r1-0528:free",
                    "deepseek/deepseek-r1-0528:free"
                ]
            
            print(f"ğŸ” æ¨¡å‹æ¯”è¾ƒæµ‹è¯•: {args.models}")
            print(f"æµ‹è¯•å¥—ä»¶: {args.test_suite}")
            
            results = runner.run_model_comparison(args.models, args.test_suite)
            runner.print_summary(results)
            
            if args.output:
                runner.save_results(results, args.output)
        
        elif args.mode == "benchmark":
            if not args.models:
                args.models = [
                    "deepseek/deepseek-r1-0528:free",
                    "deepseek/deepseek-r1-0528:free"
                ]
            
            print(f"âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•: {args.models}")
            print(f"è¿­ä»£æ¬¡æ•°: {args.iterations}")
            
            results = runner.run_performance_benchmark(args.models, args.iterations)
            runner.print_summary(results)
            
            if args.output:
                runner.save_results(results, args.output)
        
        print("\nâœ… æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()