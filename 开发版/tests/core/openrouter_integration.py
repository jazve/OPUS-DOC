#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”Œ OpenRouter API é›†æˆæ¨¡å—
æä¾›ä¸OpenRouter APIçš„é›†æˆï¼Œæ”¯æŒå¤šæ¨¡å‹æµ‹è¯•å’ŒAIè¾…åŠ©è¯„ä¼°
"""

import json
import time
import requests
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®æ•°æ®ç»“æ„"""
    id: str
    name: str
    description: str
    cost_per_token: float
    context_limit: int
    recommended_for: List[str]

@dataclass
class APIResponse:
    """APIå“åº”æ•°æ®ç»“æ„"""
    content: str
    model: str
    usage: Dict[str, Any]
    cost: float
    response_time: float
    timestamp: str
    success: bool
    error_message: Optional[str] = None

class CostTracker:
    """æˆæœ¬è¿½è¸ªå™¨"""
    
    def __init__(self):
        self.total_cost = 0.0
        self.daily_cost = 0.0
        self.requests_count = 0
        self.cost_history = []
        self.daily_budget = 20.0
        self.max_cost_per_test = 0.50
    
    def add_cost(self, cost: float, model: str, tokens: int):
        """æ·»åŠ æˆæœ¬è®°å½•"""
        self.total_cost += cost
        self.daily_cost += cost
        self.requests_count += 1
        
        cost_record = {
            'timestamp': datetime.now().isoformat(),
            'model': model,
            'cost': cost,
            'tokens': tokens,
            'cumulative_cost': self.total_cost
        }
        
        self.cost_history.append(cost_record)
        
        # æ£€æŸ¥é¢„ç®—è­¦å‘Š
        if self.daily_cost > self.daily_budget * 0.8:
            logger.warning(f"Daily cost warning: ${self.daily_cost:.4f} (80% of budget)")
        
        if cost > self.max_cost_per_test:
            logger.warning(f"High cost test: ${cost:.4f} for model {model}")
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–æˆæœ¬æ‘˜è¦"""
        return {
            'total_cost': self.total_cost,
            'daily_cost': self.daily_cost,
            'requests_count': self.requests_count,
            'average_cost_per_request': self.total_cost / max(1, self.requests_count),
            'budget_usage': self.daily_cost / self.daily_budget,
            'cost_history': self.cost_history[-10:]  # æœ€è¿‘10æ¡è®°å½•
        }

class OpenRouterClient:
    """OpenRouter APIå®¢æˆ·ç«¯"""
    
    def __init__(self, config_path: str = "config/openrouter_config.json"):
        self.config = self._load_config(config_path)
        self.api_key = self.config['api_config']['api_key']
        self.base_url = self.config['api_config']['base_url']
        self.timeout = self.config['api_config']['timeout']
        self.max_retries = self.config['api_config']['max_retries']
        self.retry_delay = self.config['api_config']['retry_delay']
        
        self.cost_tracker = CostTracker()
        self.cost_tracker.daily_budget = self.config['cost_management']['daily_budget']
        self.cost_tracker.max_cost_per_test = self.config['cost_management']['max_cost_per_test']
        
        self.available_models = self._load_models()
        
        # éªŒè¯APIå¯†é’¥
        if self.api_key == "YOUR_API_KEY_HERE":
            logger.error("è¯·åœ¨ config/openrouter_config.json ä¸­è®¾ç½®æ‚¨çš„ OpenRouter API å¯†é’¥")
            raise ValueError("APIå¯†é’¥æœªè®¾ç½®")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            raise
    
    def _load_models(self) -> Dict[str, ModelConfig]:
        """åŠ è½½æ¨¡å‹é…ç½®"""
        models = {}
        
        for model_data in self.config['models']['test_models']:
            model_config = ModelConfig(
                id=model_data['id'],
                name=model_data['name'],
                description=model_data['description'],
                cost_per_token=model_data['cost_per_token'],
                context_limit=model_data['context_limit'],
                recommended_for=model_data['recommended_for']
            )
            models[model_data['id']] = model_config
        
        return models
    
    def _make_request(self, model: str, prompt: str, **kwargs) -> APIResponse:
        """å‘é€APIè¯·æ±‚"""
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json',
            'HTTP-Referer': 'https://github.com/your-repo',  # å¯é€‰ï¼šç”¨äºè¿½è¸ª
            'X-Title': 'Prompt Testing Framework'
        }
        
        data = {
            'model': model,
            'messages': [
                {
                    'role': 'user',
                    'content': prompt
                }
            ],
            'max_tokens': kwargs.get('max_tokens', 2000),
            'temperature': kwargs.get('temperature', 0.7),
            'top_p': kwargs.get('top_p', 0.9),
            'frequency_penalty': kwargs.get('frequency_penalty', 0),
            'presence_penalty': kwargs.get('presence_penalty', 0)
        }
        
        start_time = time.time()
        
        for attempt in range(self.max_retries):
            try:
                response = requests.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=self.timeout
                )
                
                response_time = time.time() - start_time
                
                if response.status_code == 200:
                    response_data = response.json()
                    
                    # è§£æå“åº”
                    if 'choices' in response_data and len(response_data['choices']) > 0:
                        message = response_data['choices'][0]['message']
                        content = message.get('content', '')
                        
                        # å¯¹äºDeepSeek R1ç­‰æ¨¡å‹ï¼Œä¹ŸåŒ…å«reasoningå†…å®¹
                        if 'reasoning' in message and message['reasoning']:
                            # å¦‚æœcontentä¸ºç©ºä½†æœ‰reasoningï¼Œä½¿ç”¨reasoning
                            if not content.strip():
                                content = message['reasoning']
                            # å¦‚æœä¸¤è€…éƒ½æœ‰ï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦åˆå¹¶ï¼ˆè¿™é‡Œåªä½¿ç”¨contentï¼‰
                        
                        usage = response_data.get('usage', {})
                    else:
                        logger.error(f"APIå“åº”æ ¼å¼é”™è¯¯: {response_data}")
                        return APIResponse(
                            content="",
                            model=model,
                            usage={},
                            cost=0,
                            response_time=response_time,
                            timestamp=datetime.now().isoformat(),
                            success=False,
                            error_message="APIå“åº”æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘choiceså­—æ®µ"
                        )
                    
                    # è®¡ç®—æˆæœ¬
                    total_tokens = usage.get('total_tokens', 0)
                    model_config = self.available_models.get(model)
                    cost = total_tokens * model_config.cost_per_token if model_config else 0
                    
                    # è®°å½•æˆæœ¬
                    self.cost_tracker.add_cost(cost, model, total_tokens)
                    
                    # æ—¥å¿—è®°å½•
                    if self.config['logging']['log_api_calls']:
                        logger.info(f"APIè°ƒç”¨æˆåŠŸ: {model}, è€—æ—¶: {response_time:.2f}s, æˆæœ¬: ${cost:.6f}")
                    
                    return APIResponse(
                        content=content,
                        model=model,
                        usage=usage,
                        cost=cost,
                        response_time=response_time,
                        timestamp=datetime.now().isoformat(),
                        success=True
                    )
                
                else:
                    error_msg = f"APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    
                    if attempt < self.max_retries - 1:
                        logger.info(f"é‡è¯•ç¬¬ {attempt + 1} æ¬¡...")
                        time.sleep(self.retry_delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿
                        continue
                    
                    return APIResponse(
                        content="",
                        model=model,
                        usage={},
                        cost=0,
                        response_time=response_time,
                        timestamp=datetime.now().isoformat(),
                        success=False,
                        error_message=error_msg
                    )
            
            except requests.exceptions.Timeout:
                error_msg = f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries})"
                logger.error(error_msg)
                
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay * (2 ** attempt))
                    continue
                
                return APIResponse(
                    content="",
                    model=model,
                    usage={},
                    cost=0,
                    response_time=time.time() - start_time,
                    timestamp=datetime.now().isoformat(),
                    success=False,
                    error_message=error_msg
                )
            
            except Exception as e:
                error_msg = f"è¯·æ±‚å¼‚å¸¸: {str(e)}"
                logger.error(error_msg)
                
                return APIResponse(
                    content="",
                    model=model,
                    usage={},
                    cost=0,
                    response_time=time.time() - start_time,
                    timestamp=datetime.now().isoformat(),
                    success=False,
                    error_message=error_msg
                )
    
    def generate_response(self, model: str, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆå“åº”ï¼ˆç®€åŒ–æ¥å£ï¼‰"""
        response = self._make_request(model, prompt, **kwargs)
        
        if response.success:
            return response.content
        else:
            logger.error(f"ç”Ÿæˆå“åº”å¤±è´¥: {response.error_message}")
            return f"é”™è¯¯: {response.error_message}"
    
    def get_model_info(self, model_id: str) -> Optional[ModelConfig]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.available_models.get(model_id)
    
    def list_available_models(self) -> List[ModelConfig]:
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        return list(self.available_models.values())
    
    def get_recommended_models(self, task_type: str) -> List[ModelConfig]:
        """è·å–æ¨èæ¨¡å‹"""
        recommended = []
        
        for model in self.available_models.values():
            if task_type in model.recommended_for:
                recommended.append(model)
        
        return recommended
    
    def get_cost_summary(self) -> Dict[str, Any]:
        """è·å–æˆæœ¬æ‘˜è¦"""
        return self.cost_tracker.get_summary()

class AIEvaluator:
    """AIè¾…åŠ©è¯„ä¼°å™¨"""
    
    def __init__(self, client: OpenRouterClient):
        self.client = client
        self.config = client.config
        self.evaluation_model = self.config['evaluation_settings']['evaluation_model']
        self.evaluation_criteria = self.config['evaluation_settings']['evaluation_criteria']
    
    def evaluate_response(self, prompt: str, response: str, criteria: str = "content_quality") -> Dict[str, Any]:
        """è¯„ä¼°å“åº”è´¨é‡"""
        if not self.config['evaluation_settings']['enable_ai_evaluation']:
            return {'score': 0.5, 'evaluation_method': 'disabled'}
        
        criterion_config = self.evaluation_criteria.get(criteria, {})
        
        if not criterion_config.get('ai_assisted', False):
            return {'score': 0.5, 'evaluation_method': 'rule_based'}
        
        evaluation_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¯„ä¼°è¿™ä¸ªAIå“åº”çš„è´¨é‡:

åŸå§‹æç¤º: {prompt}

AIå“åº”: {response}

è¯„ä¼°æ ‡å‡†: {criterion_config.get('prompt_template', 'è¯„ä¼°æ•´ä½“è´¨é‡')}

è¯·æä¾›:
1. æ•°å€¼è¯„åˆ† (0-1ä¹‹é—´ï¼Œä¿ç•™3ä½å°æ•°)
2. ç®€çŸ­çš„è¯„ä¼°ç†ç”± (1-2å¥è¯)
3. æ”¹è¿›å»ºè®® (å¦‚æœé€‚ç”¨)

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœ:
{{
    "score": 0.xxx,
    "reason": "è¯„ä¼°ç†ç”±",
    "suggestions": "æ”¹è¿›å»ºè®®"
}}
"""
        
        try:
            eval_response = self.client.generate_response(
                self.evaluation_model,
                evaluation_prompt,
                temperature=0.3,
                max_tokens=500
            )
            
            # å°è¯•è§£æJSONç»“æœ
            try:
                result = json.loads(eval_response)
                result['evaluation_method'] = 'ai_assisted'
                result['evaluator_model'] = self.evaluation_model
                return result
            except json.JSONDecodeError:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•æå–åˆ†æ•°
                import re
                score_match = re.search(r'"score":\s*([0-9.]+)', eval_response)
                if score_match:
                    score = float(score_match.group(1))
                    return {
                        'score': min(1.0, max(0.0, score)),
                        'reason': 'AIè¯„ä¼°å®Œæˆ',
                        'suggestions': 'è¯¦è§å®Œæ•´è¯„ä¼°',
                        'evaluation_method': 'ai_assisted',
                        'evaluator_model': self.evaluation_model,
                        'raw_response': eval_response
                    }
                else:
                    return {
                        'score': 0.5,
                        'reason': 'AIè¯„ä¼°æ ¼å¼é”™è¯¯',
                        'evaluation_method': 'fallback',
                        'raw_response': eval_response
                    }
        
        except Exception as e:
            logger.error(f"AIè¯„ä¼°å¤±è´¥: {e}")
            return {
                'score': 0.5,
                'reason': f'AIè¯„ä¼°å¼‚å¸¸: {str(e)}',
                'evaluation_method': 'error_fallback'
            }
    
    def batch_evaluate(self, test_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡è¯„ä¼°"""
        evaluated_results = []
        
        for result in test_results:
            enhanced_result = result.copy()
            
            # ä¸ºæ¯ä¸ªè¯„ä¼°æ ‡å‡†è¿›è¡ŒAIè¯„ä¼°
            ai_evaluations = {}
            
            for criterion in self.evaluation_criteria.keys():
                if self.evaluation_criteria[criterion].get('ai_assisted', False):
                    evaluation = self.evaluate_response(
                        result.get('prompt', ''),
                        result.get('response', ''),
                        criterion
                    )
                    ai_evaluations[criterion] = evaluation
            
            enhanced_result['ai_evaluations'] = ai_evaluations
            evaluated_results.append(enhanced_result)
        
        return evaluated_results

class MultiModelTester:
    """å¤šæ¨¡å‹æµ‹è¯•å™¨"""
    
    def __init__(self, client: OpenRouterClient):
        self.client = client
        self.evaluator = AIEvaluator(client)
    
    def compare_models(self, models: List[str], prompt: str, **kwargs) -> Dict[str, Any]:
        """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„å“åº”"""
        results = {}
        
        for model in models:
            logger.info(f"æµ‹è¯•æ¨¡å‹: {model}")
            
            start_time = time.time()
            response = self.client._make_request(model, prompt, **kwargs)
            
            results[model] = {
                'response': response.content,
                'success': response.success,
                'response_time': response.response_time,
                'cost': response.cost,
                'usage': response.usage,
                'error': response.error_message
            }
            
            # å¦‚æœå¯ç”¨äº†AIè¯„ä¼°
            if self.client.config['evaluation_settings']['enable_ai_evaluation']:
                evaluation = self.evaluator.evaluate_response(prompt, response.content)
                results[model]['ai_evaluation'] = evaluation
        
        return results
    
    def performance_benchmark(self, models: List[str], test_prompts: List[str]) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        benchmark_results = {
            'summary': {},
            'detailed_results': [],
            'cost_analysis': {}
        }
        
        for i, prompt in enumerate(test_prompts):
            logger.info(f"æ‰§è¡ŒåŸºå‡†æµ‹è¯• {i+1}/{len(test_prompts)}")
            
            test_result = self.compare_models(models, prompt)
            test_result['prompt_id'] = i
            test_result['prompt'] = prompt[:100] + "..." if len(prompt) > 100 else prompt
            
            benchmark_results['detailed_results'].append(test_result)
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        for model in models:
            model_stats = {
                'total_tests': len(test_prompts),
                'successful_tests': 0,
                'total_cost': 0,
                'avg_response_time': 0,
                'avg_ai_score': 0
            }
            
            response_times = []
            ai_scores = []
            
            for result in benchmark_results['detailed_results']:
                if model in result and result[model]['success']:
                    model_stats['successful_tests'] += 1
                    model_stats['total_cost'] += result[model]['cost']
                    response_times.append(result[model]['response_time'])
                    
                    if 'ai_evaluation' in result[model]:
                        ai_scores.append(result[model]['ai_evaluation'].get('score', 0))
            
            if response_times:
                model_stats['avg_response_time'] = sum(response_times) / len(response_times)
            
            if ai_scores:
                model_stats['avg_ai_score'] = sum(ai_scores) / len(ai_scores)
            
            model_stats['success_rate'] = model_stats['successful_tests'] / model_stats['total_tests']
            
            benchmark_results['summary'][model] = model_stats
        
        # æˆæœ¬åˆ†æ
        benchmark_results['cost_analysis'] = self.client.get_cost_summary()
        
        return benchmark_results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = OpenRouterClient()
    
    # åˆ—å‡ºå¯ç”¨æ¨¡å‹
    models = client.list_available_models()
    print(f"å¯ç”¨æ¨¡å‹: {[m.name for m in models]}")
    
    # æµ‹è¯•å•ä¸ªæ¨¡å‹
    test_prompt = "è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Œå¹¶ä¸¾ä¾‹è¯´æ˜å…¶åº”ç”¨ã€‚"
    response = client.generate_response("deepseek/deepseek-r1-0528:free", test_prompt)
    print(f"å“åº”: {response[:100]}...")
    
    # å¤šæ¨¡å‹æ¯”è¾ƒ
    tester = MultiModelTester(client)
    comparison = tester.compare_models(
        ["deepseek/deepseek-r1-0528:free", "deepseek/deepseek-r1-0528:free"],
        test_prompt
    )
    
    print("\næ¨¡å‹æ¯”è¾ƒç»“æœ:")
    for model, result in comparison.items():
        print(f"{model}: æˆåŠŸ={result['success']}, æ—¶é—´={result['response_time']:.2f}s, æˆæœ¬=${result['cost']:.6f}")
    
    # æ˜¾ç¤ºæˆæœ¬æ‘˜è¦
    cost_summary = client.get_cost_summary()
    print(f"\næˆæœ¬æ‘˜è¦: ${cost_summary['total_cost']:.6f}")