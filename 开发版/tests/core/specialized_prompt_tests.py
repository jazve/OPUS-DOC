#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ ä¸“é—¨åŒ–æç¤ºè¯æµ‹è¯•å¥—ä»¶
é’ˆå¯¹ä¸åŒç±»å‹çš„æç¤ºè¯è®¾è®¡ä¸“é—¨çš„æµ‹è¯•æ–¹æ³•å’Œè¯„ä¼°æ ‡å‡†
"""

import re
import time
from typing import Callable, Dict, List, Any
from prompt_testing_framework import TestCase, TestResult, PromptTestingFramework
import logging

logger = logging.getLogger(__name__)

class SSAPFrameworkTester(PromptTestingFramework):
    """SSAPæ¡†æ¶ä¸“é—¨æµ‹è¯•å™¨"""
    
    def __init__(self):
        super().__init__()
        self.ssap_patterns = {
            'role_definition': r'(æˆ‘æ˜¯|ä½œä¸º|èº«ä»½[:ï¼š]|ä¸“å®¶|åŠ©æ‰‹)',
            'knowledge_base': r'(çŸ¥è¯†åº“|Knowledge|å…·å¤‡.*çŸ¥è¯†|ä¸“ä¸šçŸ¥è¯†)',
            'skills_tools': r'(æŠ€èƒ½|Skills|å·¥å…·|Tools|èƒ½åŠ›)',
            'format_output': r'(Format|æ ¼å¼|è¾“å‡ºæ ‡å‡†|ğŸ¯|ğŸ’¡|ğŸš€|ğŸ“Š)',
            'workflow_logic': r'(workflow|å·¥ä½œæµ|FUNCTION|BEGIN|END|IF.*THEN)',
            'memory_system': r'(Memory|è®°å¿†|å†å²|ç»éªŒ|å­¦ä¹ )'
        }
        
        self.quality_metrics = {
            'role_clarity': 0.0,
            'knowledge_coverage': 0.0,
            'skill_integration': 0.0,
            'format_consistency': 0.0,
            'workflow_logic': 0.0,
            'memory_utilization': 0.0
        }
    
    def evaluate_ssap_completeness(self, response: str) -> Dict[str, float]:
        """è¯„ä¼°SSAPæ¡†æ¶å®Œæ•´æ€§"""
        scores = {}
        
        for component, pattern in self.ssap_patterns.items():
            matches = len(re.findall(pattern, response, re.IGNORECASE))
            # åŸºäºåŒ¹é…æ•°é‡å’Œå“åº”é•¿åº¦è®¡ç®—åˆ†æ•°
            score = min(1.0, matches / 2.0) if matches > 0 else 0.0
            scores[component] = score
        
        return scores
    
    def test_role_consistency(self, test_cases: List[TestCase], ai_response_func: Callable) -> Dict[str, Any]:
        """æµ‹è¯•è§’è‰²ä¸€è‡´æ€§"""
        results = []
        
        for test_case in test_cases:
            responses = []
            for i in range(3):  # å¤šæ¬¡æµ‹è¯•ç¡®ä¿ä¸€è‡´æ€§
                response = ai_response_func(test_case.input_text)
                responses.append(response)
            
            # åˆ†æä¸€è‡´æ€§
            role_mentions = []
            for response in responses:
                roles = re.findall(r'(æˆ‘æ˜¯|ä½œä¸º)([^ï¼Œã€‚,\.]+)', response)
                role_mentions.extend([role[1].strip() for role in roles])
            
            consistency_score = 1.0 if len(set(role_mentions)) <= 1 else 0.5
            
            results.append({
                'test_id': test_case.id,
                'consistency_score': consistency_score,
                'role_mentions': role_mentions,
                'responses': responses
            })
        
        return {
            'average_consistency': sum(r['consistency_score'] for r in results) / len(results),
            'details': results
        }
    
    def test_format_adherence(self, test_cases: List[TestCase], ai_response_func: Callable) -> Dict[str, Any]:
        """æµ‹è¯•æ ¼å¼éµå¾ªåº¦"""
        format_tests = {
            'four_section_format': r'ğŸ¯.*ğŸ’¡.*ğŸš€.*ğŸ“Š',
            'workflow_format': r'<workflow>.*FUNCTION.*BEGIN.*END.*</workflow>',
            'structured_sections': r'(##|###|\*\*.*\*\*|ã€.*ã€‘)',
            'bullet_points': r'[-â€¢*]\s+',
            'numbered_lists': r'\d+\.\s+'
        }
        
        results = []
        for test_case in test_cases:
            response = ai_response_func(test_case.input_text)
            
            format_scores = {}
            for format_name, pattern in format_tests.items():
                matches = bool(re.search(pattern, response, re.DOTALL))
                format_scores[format_name] = 1.0 if matches else 0.0
            
            results.append({
                'test_id': test_case.id,
                'format_scores': format_scores,
                'overall_format_score': sum(format_scores.values()) / len(format_scores)
            })
        
        return {
            'average_format_compliance': sum(r['overall_format_score'] for r in results) / len(results),
            'details': results
        }

class WorkflowTester(PromptTestingFramework):
    """å·¥ä½œæµæç¤ºè¯æµ‹è¯•å™¨"""
    
    def __init__(self):
        super().__init__()
        self.workflow_elements = {
            'function_definition': r'FUNCTION\s+\w+\s*\(',
            'begin_end_blocks': r'BEGIN.*END',
            'conditional_logic': r'IF\s+.*THEN',
            'variable_assignment': r'\w+\s*=\s*',
            'return_statement': r'RETURN\s+',
            'loop_structures': r'(FOR|WHILE)\s+',
            'error_handling': r'(TRY|CATCH|ERROR)',
            'stop_wait_control': r'STOP_AND_WAIT'
        }
    
    def analyze_workflow_complexity(self, workflow_code: str) -> Dict[str, Any]:
        """åˆ†æå·¥ä½œæµå¤æ‚åº¦"""
        complexity_metrics = {
            'cyclomatic_complexity': 1,  # åŸºç¡€å¤æ‚åº¦
            'nesting_depth': 0,
            'decision_points': 0,
            'function_calls': 0,
            'variable_usage': 0
        }
        
        # è®¡ç®—å†³ç­–ç‚¹æ•°é‡
        decision_patterns = [r'IF\s+', r'ELSEIF\s+', r'ELSE\s+', r'SWITCH\s+', r'CASE\s+']
        for pattern in decision_patterns:
            matches = len(re.findall(pattern, workflow_code, re.IGNORECASE))
            complexity_metrics['decision_points'] += matches
            complexity_metrics['cyclomatic_complexity'] += matches
        
        # è®¡ç®—åµŒå¥—æ·±åº¦
        depth = 0
        max_depth = 0
        for line in workflow_code.split('\n'):
            if re.search(r'(IF|FOR|WHILE|FUNCTION)', line, re.IGNORECASE):
                depth += 1
                max_depth = max(max_depth, depth)
            elif re.search(r'(END|ENDIF)', line, re.IGNORECASE):
                depth = max(0, depth - 1)
        
        complexity_metrics['nesting_depth'] = max_depth
        
        # è®¡ç®—å‡½æ•°è°ƒç”¨æ•°é‡
        complexity_metrics['function_calls'] = len(re.findall(r'\w+\s*\(', workflow_code))
        
        # è®¡ç®—å˜é‡ä½¿ç”¨æ•°é‡
        complexity_metrics['variable_usage'] = len(re.findall(r'\w+\s*=', workflow_code))
        
        return complexity_metrics
    
    def test_workflow_validity(self, test_cases: List[TestCase], ai_response_func: Callable) -> Dict[str, Any]:
        """æµ‹è¯•å·¥ä½œæµæœ‰æ•ˆæ€§"""
        results = []
        
        for test_case in test_cases:
            response = ai_response_func(test_case.input_text)
            
            # æ£€æŸ¥å·¥ä½œæµç»“æ„
            structure_scores = {}
            for element, pattern in self.workflow_elements.items():
                matches = bool(re.search(pattern, response, re.IGNORECASE | re.DOTALL))
                structure_scores[element] = 1.0 if matches else 0.0
            
            # æ£€æŸ¥è¯­æ³•ä¸€è‡´æ€§
            syntax_score = self._check_workflow_syntax(response)
            
            # åˆ†æå¤æ‚åº¦
            complexity = self.analyze_workflow_complexity(response)
            
            overall_score = (
                sum(structure_scores.values()) / len(structure_scores) * 0.6 +
                syntax_score * 0.4
            )
            
            results.append({
                'test_id': test_case.id,
                'structure_scores': structure_scores,
                'syntax_score': syntax_score,
                'complexity_metrics': complexity,
                'overall_score': overall_score,
                'workflow_code': response
            })
        
        return {
            'average_validity_score': sum(r['overall_score'] for r in results) / len(results),
            'details': results
        }
    
    def _check_workflow_syntax(self, workflow_code: str) -> float:
        """æ£€æŸ¥å·¥ä½œæµè¯­æ³•"""
        syntax_checks = {
            'balanced_begin_end': 0,
            'proper_indentation': 0,
            'function_closure': 0,
            'variable_consistency': 0
        }
        
        # æ£€æŸ¥BEGIN/ENDå¹³è¡¡
        begin_count = len(re.findall(r'BEGIN', workflow_code, re.IGNORECASE))
        end_count = len(re.findall(r'END', workflow_code, re.IGNORECASE))
        syntax_checks['balanced_begin_end'] = 1.0 if begin_count == end_count else 0.0
        
        # æ£€æŸ¥å‡½æ•°å®šä¹‰ä¸ç»“æŸ
        function_count = len(re.findall(r'FUNCTION\s+\w+', workflow_code, re.IGNORECASE))
        syntax_checks['function_closure'] = 1.0 if function_count > 0 and begin_count >= function_count else 0.0
        
        # ç®€å•çš„ç¼©è¿›æ£€æŸ¥
        lines = workflow_code.split('\n')
        proper_indent = sum(1 for line in lines if line.strip() and (line.startswith('  ') or not line.startswith(' ')))
        syntax_checks['proper_indentation'] = min(1.0, proper_indent / max(1, len([l for l in lines if l.strip()])))
        
        return sum(syntax_checks.values()) / len(syntax_checks)

class PersonaTester(PromptTestingFramework):
    """äººæ ¼åŒ–æç¤ºè¯æµ‹è¯•å™¨"""
    
    def __init__(self):
        super().__init__()
        self.persona_aspects = {
            'personality_traits': r'(æ€§æ ¼|ç‰¹ç‚¹|é£æ ¼|ä¸ªæ€§|æ€åº¦)',
            'communication_style': r'(è¯­è¨€|è¡¨è¾¾|æ²Ÿé€š|ç”¨è¯|è¯­è°ƒ)',
            'expertise_domain': r'(ä¸“ä¸š|é¢†åŸŸ|ç»éªŒ|ä¸“é•¿|æ“…é•¿)',
            'behavioral_patterns': r'(è¡Œä¸º|ä¹ æƒ¯|æ–¹å¼|æ–¹æ³•|åšæ³•)',
            'emotional_tone': r'(æƒ…æ„Ÿ|æƒ…ç»ª|æ„Ÿå—|ä½“éªŒ|æ°›å›´)'
        }
    
    def test_persona_consistency(self, test_cases: List[TestCase], ai_response_func: Callable) -> Dict[str, Any]:
        """æµ‹è¯•äººæ ¼ä¸€è‡´æ€§"""
        results = []
        
        for test_case in test_cases:
            # å¤šæ¬¡æµ‹è¯•åŒä¸€è¾“å…¥
            responses = []
            for _ in range(3):
                response = ai_response_func(test_case.input_text)
                responses.append(response)
            
            # åˆ†æäººæ ¼ç‰¹å¾ä¸€è‡´æ€§
            consistency_scores = {}
            for aspect, pattern in self.persona_aspects.items():
                aspect_mentions = []
                for response in responses:
                    matches = re.findall(pattern, response, re.IGNORECASE)
                    aspect_mentions.extend(matches)
                
                # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
                unique_mentions = set(aspect_mentions)
                consistency_score = 1.0 - (len(unique_mentions) / max(1, len(aspect_mentions)))
                consistency_scores[aspect] = consistency_score
            
            results.append({
                'test_id': test_case.id,
                'consistency_scores': consistency_scores,
                'overall_consistency': sum(consistency_scores.values()) / len(consistency_scores),
                'responses': responses
            })
        
        return {
            'average_persona_consistency': sum(r['overall_consistency'] for r in results) / len(results),
            'details': results
        }
    
    def analyze_communication_style(self, response: str) -> Dict[str, float]:
        """åˆ†ææ²Ÿé€šé£æ ¼"""
        style_metrics = {
            'formality': 0.0,
            'friendliness': 0.0,
            'technical_depth': 0.0,
            'empathy': 0.0,
            'confidence': 0.0
        }
        
        # æ­£å¼åº¦åˆ†æ
        formal_indicators = ['æ‚¨', 'è¯·', 'å»ºè®®', 'åˆ†æ', 'è¯„ä¼°', 'å…·ä½“', 'è¯¦ç»†']
        informal_indicators = ['ä½ ', 'å’±ä»¬', 'å¥½çš„', 'æ²¡é—®é¢˜', 'å½“ç„¶', 'è‚¯å®š']
        
        formal_count = sum(1 for indicator in formal_indicators if indicator in response)
        informal_count = sum(1 for indicator in informal_indicators if indicator in response)
        
        style_metrics['formality'] = formal_count / max(1, formal_count + informal_count)
        
        # å‹å¥½åº¦åˆ†æ
        friendly_indicators = ['å¾ˆé«˜å…´', 'æ„¿æ„', 'å¸®åŠ©', 'æ”¯æŒ', 'ç†è§£', 'æ„Ÿè°¢']
        friendly_count = sum(1 for indicator in friendly_indicators if indicator in response)
        style_metrics['friendliness'] = min(1.0, friendly_count / 3)
        
        # æŠ€æœ¯æ·±åº¦
        technical_indicators = ['ç®—æ³•', 'æ¨¡å‹', 'æ¡†æ¶', 'æ¶æ„', 'å®ç°', 'ä¼˜åŒ–', 'æ€§èƒ½']
        technical_count = sum(1 for indicator in technical_indicators if indicator in response)
        style_metrics['technical_depth'] = min(1.0, technical_count / 5)
        
        # åŒç†å¿ƒ
        empathy_indicators = ['ç†è§£', 'æ„Ÿå—', 'ä½“éªŒ', 'å›°éš¾', 'æŒ‘æˆ˜', 'æ‹…å¿ƒ']
        empathy_count = sum(1 for indicator in empathy_indicators if indicator in response)
        style_metrics['empathy'] = min(1.0, empathy_count / 3)
        
        # ä¿¡å¿ƒåº¦
        confidence_indicators = ['ç¡®ä¿¡', 'è‚¯å®š', 'æ˜ç¡®', 'ä¸€å®š', 'å¿…ç„¶', 'æ˜¾ç„¶']
        confidence_count = sum(1 for indicator in confidence_indicators if indicator in response)
        style_metrics['confidence'] = min(1.0, confidence_count / 3)
        
        return style_metrics

class PerformanceTester(PromptTestingFramework):
    """æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        super().__init__()
        self.performance_metrics = {
            'response_time': [],
            'token_efficiency': [],
            'memory_usage': [],
            'consistency_score': [],
            'throughput': 0
        }
    
    def benchmark_response_time(self, test_cases: List[TestCase], ai_response_func: Callable, iterations: int = 5) -> Dict[str, Any]:
        """åŸºå‡†å“åº”æ—¶é—´æµ‹è¯•"""
        results = []
        
        for test_case in test_cases:
            times = []
            for _ in range(iterations):
                start_time = time.time()
                response = ai_response_func(test_case.input_text)
                end_time = time.time()
                times.append(end_time - start_time)
            
            avg_time = sum(times) / len(times)
            min_time = min(times)
            max_time = max(times)
            
            results.append({
                'test_id': test_case.id,
                'average_time': avg_time,
                'min_time': min_time,
                'max_time': max_time,
                'times': times,
                'consistency': 1.0 - (max_time - min_time) / max_time if max_time > 0 else 1.0
            })
        
        return {
            'overall_average_time': sum(r['average_time'] for r in results) / len(results),
            'overall_consistency': sum(r['consistency'] for r in results) / len(results),
            'details': results
        }
    
    def stress_test(self, test_case: TestCase, ai_response_func: Callable, concurrent_requests: int = 10) -> Dict[str, Any]:
        """å‹åŠ›æµ‹è¯•"""
        import threading
        import queue
        
        result_queue = queue.Queue()
        
        def worker():
            try:
                start_time = time.time()
                response = ai_response_func(test_case.input_text)
                end_time = time.time()
                result_queue.put({
                    'success': True,
                    'response_time': end_time - start_time,
                    'response_length': len(response)
                })
            except Exception as e:
                result_queue.put({
                    'success': False,
                    'error': str(e),
                    'response_time': 0
                })
        
        # å¯åŠ¨å¹¶å‘è¯·æ±‚
        threads = []
        start_time = time.time()
        
        for _ in range(concurrent_requests):
            thread = threading.Thread(target=worker)
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        total_time = time.time() - start_time
        
        # æ”¶é›†ç»“æœ
        results = []
        while not result_queue.empty():
            results.append(result_queue.get())
        
        successful_requests = [r for r in results if r['success']]
        failed_requests = [r for r in results if not r['success']]
        
        return {
            'total_requests': concurrent_requests,
            'successful_requests': len(successful_requests),
            'failed_requests': len(failed_requests),
            'success_rate': len(successful_requests) / concurrent_requests,
            'average_response_time': sum(r['response_time'] for r in successful_requests) / len(successful_requests) if successful_requests else 0,
            'throughput': len(successful_requests) / total_time,
            'total_time': total_time,
            'errors': [r['error'] for r in failed_requests]
        }

# æµ‹è¯•å¥—ä»¶ç”Ÿæˆå™¨
class TestSuiteGenerator:
    """æµ‹è¯•å¥—ä»¶ç”Ÿæˆå™¨"""
    
    @staticmethod
    def create_ssap_test_suite() -> List[TestCase]:
        """åˆ›å»ºSSAPæ¡†æ¶æµ‹è¯•å¥—ä»¶"""
        return [
            TestCase(
                id="ssap_001",
                name="è§’è‰²å®šä¹‰æµ‹è¯•",
                input_text="ä½ æ˜¯è°ï¼Ÿ",
                expected_patterns=["æˆ‘æ˜¯", "ä¸“å®¶", "åŠ©æ‰‹", "èº«ä»½"],
                expected_format="",
                category="role_definition",
                priority="high",
                metadata={"type": "identity_test"}
            ),
            TestCase(
                id="ssap_002",
                name="å››æ®µå¼è¾“å‡ºæµ‹è¯•",
                input_text="å¸®æˆ‘åˆ†æä¸€ä¸ªè¥é”€ç­–ç•¥",
                expected_patterns=["ğŸ¯", "ğŸ’¡", "ğŸš€", "ğŸ“Š"],
                expected_format="ssap_4_section",
                category="format_compliance",
                priority="high",
                metadata={"type": "format_test"}
            ),
            TestCase(
                id="ssap_003",
                name="å·¥ä½œæµç”Ÿæˆæµ‹è¯•",
                input_text="è®¾è®¡ä¸€ä¸ªæ•°æ®å¤„ç†å·¥ä½œæµ",
                expected_patterns=["FUNCTION", "BEGIN", "END", "workflow"],
                expected_format="workflow_format",
                category="workflow_generation",
                priority="medium",
                metadata={"type": "workflow_test"}
            )
        ]
    
    @staticmethod
    def create_performance_test_suite() -> List[TestCase]:
        """åˆ›å»ºæ€§èƒ½æµ‹è¯•å¥—ä»¶"""
        return [
            TestCase(
                id="perf_001",
                name="çŸ­å“åº”æ—¶é—´æµ‹è¯•",
                input_text="ä½ å¥½",
                expected_patterns=[],
                expected_format="",
                category="performance",
                priority="medium",
                metadata={"expected_max_time": 2.0}
            ),
            TestCase(
                id="perf_002",
                name="å¤æ‚ä»»åŠ¡å“åº”æ—¶é—´æµ‹è¯•",
                input_text="è¯·è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„ç”µå•†ç³»ç»Ÿæ¶æ„ï¼ŒåŒ…æ‹¬å‰ç«¯ã€åç«¯ã€æ•°æ®åº“ã€ç¼“å­˜ã€æ¶ˆæ¯é˜Ÿåˆ—ç­‰ç»„ä»¶",
                expected_patterns=[],
                expected_format="",
                category="performance",
                priority="high",
                metadata={"expected_max_time": 10.0}
            )
        ]
    
    @staticmethod
    def create_consistency_test_suite() -> List[TestCase]:
        """åˆ›å»ºä¸€è‡´æ€§æµ‹è¯•å¥—ä»¶"""
        return [
            TestCase(
                id="consistency_001",
                name="è§’è‰²ä¸€è‡´æ€§æµ‹è¯•",
                input_text="ä»‹ç»ä¸€ä¸‹ä½ çš„èƒ½åŠ›",
                expected_patterns=["ä¸“ä¸š", "èƒ½åŠ›", "æœåŠ¡"],
                expected_format="",
                category="consistency",
                priority="high",
                metadata={"repeat_count": 5}
            )
        ]

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    def mock_ai_response(prompt: str) -> str:
        if "å·¥ä½œæµ" in prompt:
            return """<workflow>
FUNCTION æ•°æ®å¤„ç†å·¥ä½œæµ(è¾“å…¥æ•°æ®):
BEGIN
  æ•°æ®éªŒè¯ = éªŒè¯æ•°æ®æ ¼å¼(è¾“å…¥æ•°æ®)
  IF æ•°æ®éªŒè¯ == "é€šè¿‡" THEN:
    æ¸…æ´—æ•°æ® = æ•°æ®æ¸…æ´—(è¾“å…¥æ•°æ®)
    å¤„ç†ç»“æœ = æ•°æ®å¤„ç†(æ¸…æ´—æ•°æ®)
    RETURN å¤„ç†ç»“æœ
  ELSE:
    RETURN é”™è¯¯ä¿¡æ¯
  END
END
</workflow>"""
        elif "ä½ æ˜¯è°" in prompt:
            return "æˆ‘æ˜¯ä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œä¸“æ³¨äºä¸ºæ‚¨æä¾›é«˜è´¨é‡çš„æœåŠ¡ã€‚"
        else:
            return "ğŸ¯ éœ€æ±‚ç†è§£\nğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ\nğŸš€ è§£å†³æ–¹æ¡ˆ\nğŸ“Š è¡¥å……ä¿¡æ¯"
    
    # æµ‹è¯•SSAPæ¡†æ¶
    ssap_tester = SSAPFrameworkTester()
    test_cases = TestSuiteGenerator.create_ssap_test_suite()
    
    for test_case in test_cases:
        ssap_tester.add_test_case(test_case)
    
    # è¿è¡Œæµ‹è¯•
    summary = ssap_tester.run_all_tests(mock_ai_response)
    print(f"SSAPæµ‹è¯•ç»“æœ: {summary}")
    
    # æµ‹è¯•æ ¼å¼éµå¾ªåº¦
    format_result = ssap_tester.test_format_adherence(test_cases, mock_ai_response)
    print(f"æ ¼å¼éµå¾ªåº¦: {format_result['average_format_compliance']:.2%}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = ssap_tester.generate_report("ssap_test_report.json")
    print("æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ")