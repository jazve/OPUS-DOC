#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“Š æç¤ºè¯è¯„ä¼°æŒ‡æ ‡ç³»ç»Ÿ
æä¾›å¤šç»´åº¦çš„æç¤ºè¯è´¨é‡è¯„ä¼°å’Œæ‰“åˆ†æœºåˆ¶
"""

import re
import statistics
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœæ•°æ®ç»“æ„"""
    metric_name: str
    score: float
    max_score: float
    details: Dict[str, Any]
    timestamp: str

class MetricCalculator:
    """æŒ‡æ ‡è®¡ç®—å™¨åŸºç±»"""
    
    def __init__(self, name: str, weight: float = 1.0):
        self.name = name
        self.weight = weight
    
    def calculate(self, response: str, expected: Optional[Dict[str, Any]] = None) -> EvaluationResult:
        """è®¡ç®—æŒ‡æ ‡åˆ†æ•°"""
        raise NotImplementedError

class ContentQualityMetric(MetricCalculator):
    """å†…å®¹è´¨é‡æŒ‡æ ‡"""
    
    def __init__(self, weight: float = 1.0):
        super().__init__("content_quality", weight)
        self.quality_indicators = {
            'depth': {
                'patterns': [r'å…·ä½“', r'è¯¦ç»†', r'æ·±å…¥', r'å…¨é¢', r'ç³»ç»Ÿ'],
                'weight': 0.3
            },
            'accuracy': {
                'patterns': [r'å‡†ç¡®', r'æ­£ç¡®', r'ç²¾ç¡®', r'å¯é ', r'æœ‰æ•ˆ'],
                'weight': 0.25
            },
            'completeness': {
                'patterns': [r'å®Œæ•´', r'å…¨é¢', r'ç»¼åˆ', r'å®Œå–„', r'é½å…¨'],
                'weight': 0.25
            },
            'clarity': {
                'patterns': [r'æ¸…æ™°', r'æ˜ç¡®', r'ç®€æ˜', r'æ˜“æ‡‚', r'æ˜äº†'],
                'weight': 0.2
            }
        }
    
    def calculate(self, response: str, expected: Optional[Dict[str, Any]] = None) -> EvaluationResult:
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°"""
        quality_scores = {}
        
        # è®¡ç®—å„ç»´åº¦åˆ†æ•°
        for dimension, config in self.quality_indicators.items():
            patterns = config['patterns']
            weight = config['weight']
            
            # ç»Ÿè®¡æ¨¡å¼åŒ¹é…
            matches = 0
            for pattern in patterns:
                matches += len(re.findall(pattern, response, re.IGNORECASE))
            
            # åŸºäºåŒ¹é…æ•°å’Œå“åº”é•¿åº¦è®¡ç®—åˆ†æ•°
            response_length = len(response)
            density = matches / max(1, response_length / 100)  # æ¯100å­—ç¬¦çš„åŒ¹é…å¯†åº¦
            score = min(1.0, density * 2)  # æ ‡å‡†åŒ–åˆ°0-1
            
            quality_scores[dimension] = {
                'score': score,
                'weight': weight,
                'matches': matches,
                'density': density
            }
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = sum(
            scores['score'] * scores['weight'] 
            for scores in quality_scores.values()
        )
        
        # é¢å¤–æ£€æŸ¥ï¼šå“åº”é•¿åº¦åˆç†æ€§
        length_score = self._calculate_length_score(response)
        total_score = total_score * 0.8 + length_score * 0.2
        
        return EvaluationResult(
            metric_name=self.name,
            score=total_score,
            max_score=1.0,
            details={
                'quality_dimensions': quality_scores,
                'length_score': length_score,
                'response_length': len(response),
                'word_count': len(response.split())
            },
            timestamp=datetime.now().isoformat()
        )
    
    def _calculate_length_score(self, response: str) -> float:
        """è®¡ç®—å“åº”é•¿åº¦åˆç†æ€§åˆ†æ•°"""
        length = len(response)
        
        # å®šä¹‰ç†æƒ³é•¿åº¦èŒƒå›´
        if 50 <= length <= 500:
            return 1.0
        elif 500 < length <= 1000:
            return 0.9
        elif 1000 < length <= 2000:
            return 0.8
        elif 20 <= length < 50:
            return 0.6
        elif length > 2000:
            return max(0.5, 1.0 - (length - 2000) / 5000)
        else:
            return 0.3

class StructureMetric(MetricCalculator):
    """ç»“æ„åŒ–æŒ‡æ ‡"""
    
    def __init__(self, weight: float = 1.0):
        super().__init__("structure", weight)
        self.structure_patterns = {
            'headers': r'(#{1,6}\s+|ã€.*?ã€‘|\*\*.*?\*\*)',
            'bullet_points': r'[-â€¢*]\s+',
            'numbered_lists': r'\d+\.\s+',
            'sections': r'(ç¬¬\d+[æ­¥ç‚¹]|æ­¥éª¤\d+|é˜¶æ®µ\d+)',
            'emphasis': r'(\*\*.*?\*\*|__.*?__|ğŸ¯|ğŸ’¡|ğŸš€|ğŸ“Š)',
            'code_blocks': r'```.*?```|`.*?`',
            'tables': r'\|.*?\|',
            'workflows': r'<workflow>.*?</workflow>'
        }
    
    def calculate(self, response: str, expected: Optional[Dict[str, Any]] = None) -> EvaluationResult:
        """è®¡ç®—ç»“æ„åŒ–åˆ†æ•°"""
        structure_scores = {}
        
        for element, pattern in self.structure_patterns.items():
            matches = len(re.findall(pattern, response, re.DOTALL | re.IGNORECASE))
            
            # æ ¹æ®å…ƒç´ ç±»å‹è®¡ç®—åˆ†æ•°
            if element in ['headers', 'sections']:
                score = min(1.0, matches / 3)  # æœŸæœ›æœ‰2-3ä¸ªæ ‡é¢˜æˆ–ç« èŠ‚
            elif element in ['bullet_points', 'numbered_lists']:
                score = min(1.0, matches / 5)  # æœŸæœ›æœ‰3-5ä¸ªåˆ—è¡¨é¡¹
            elif element in ['emphasis']:
                score = min(1.0, matches / 4)  # æœŸæœ›æœ‰é€‚é‡å¼ºè°ƒ
            else:
                score = min(1.0, matches / 2)  # å…¶ä»–å…ƒç´ 
            
            structure_scores[element] = {
                'score': score,
                'matches': matches
            }
        
        # è®¡ç®—æ€»ä½“ç»“æ„åˆ†æ•°
        total_elements = len(structure_scores)
        active_elements = sum(1 for s in structure_scores.values() if s['matches'] > 0)
        structure_diversity = active_elements / total_elements
        
        average_score = sum(s['score'] for s in structure_scores.values()) / total_elements
        
        # ç»¼åˆåˆ†æ•°ï¼šå¹³å‡åˆ† * å¤šæ ·æ€§åŠ æƒ
        final_score = average_score * 0.7 + structure_diversity * 0.3
        
        return EvaluationResult(
            metric_name=self.name,
            score=final_score,
            max_score=1.0,
            details={
                'structure_elements': structure_scores,
                'structure_diversity': structure_diversity,
                'active_elements': active_elements,
                'total_elements': total_elements
            },
            timestamp=datetime.now().isoformat()
        )

class AccuracyMetric(MetricCalculator):
    """å‡†ç¡®æ€§æŒ‡æ ‡"""
    
    def __init__(self, weight: float = 1.0):
        super().__init__("accuracy", weight)
    
    def calculate(self, response: str, expected: Optional[Dict[str, Any]] = None) -> EvaluationResult:
        """è®¡ç®—å‡†ç¡®æ€§åˆ†æ•°"""
        if not expected:
            # æ²¡æœ‰æœŸæœ›ç»“æœæ—¶ï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•
            return self._heuristic_accuracy(response)
        
        # æœ‰æœŸæœ›ç»“æœæ—¶ï¼Œè¿›è¡Œå…·ä½“åŒ¹é…
        accuracy_scores = {}
        
        # å…³é”®è¯åŒ¹é…
        if 'keywords' in expected:
            keywords = expected['keywords']
            matched_keywords = sum(1 for kw in keywords if kw.lower() in response.lower())
            accuracy_scores['keyword_match'] = matched_keywords / len(keywords)
        
        # æ¨¡å¼åŒ¹é…
        if 'patterns' in expected:
            patterns = expected['patterns']
            matched_patterns = sum(1 for pattern in patterns if re.search(pattern, response, re.IGNORECASE))
            accuracy_scores['pattern_match'] = matched_patterns / len(patterns)
        
        # æ ¼å¼åŒ¹é…
        if 'format' in expected:
            format_pattern = expected['format']
            format_match = bool(re.search(format_pattern, response, re.DOTALL | re.IGNORECASE))
            accuracy_scores['format_match'] = 1.0 if format_match else 0.0
        
        # è¯­ä¹‰ç›¸ä¼¼æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if 'semantic_target' in expected:
            semantic_score = self._calculate_semantic_similarity(response, expected['semantic_target'])
            accuracy_scores['semantic_similarity'] = semantic_score
        
        # è®¡ç®—åŠ æƒå¹³å‡
        if accuracy_scores:
            total_score = sum(accuracy_scores.values()) / len(accuracy_scores)
        else:
            total_score = 0.5  # é»˜è®¤åˆ†æ•°
        
        return EvaluationResult(
            metric_name=self.name,
            score=total_score,
            max_score=1.0,
            details=accuracy_scores,
            timestamp=datetime.now().isoformat()
        )
    
    def _heuristic_accuracy(self, response: str) -> EvaluationResult:
        """å¯å‘å¼å‡†ç¡®æ€§è¯„ä¼°"""
        accuracy_indicators = {
            'factual_language': len(re.findall(r'(æ ¹æ®|åŸºäº|æ•°æ®æ˜¾ç¤º|ç ”ç©¶è¡¨æ˜|äº‹å®ä¸Š)', response, re.IGNORECASE)),
            'uncertainty_handling': len(re.findall(r'(å¯èƒ½|ä¹Ÿè®¸|å¤§æ¦‚|ä¼°è®¡|ä¸ç¡®å®š)', response, re.IGNORECASE)),
            'logical_connectors': len(re.findall(r'(å› æ­¤|æ‰€ä»¥|ç”±äº|å› ä¸º|ç„¶è€Œ|ä½†æ˜¯|ä¸è¿‡)', response, re.IGNORECASE)),
            'specific_details': len(re.findall(r'(\d+%|\d+ä¸ª|\d+é¡¹|å…·ä½“|è¯¦ç»†)', response, re.IGNORECASE))
        }
        
        # è®¡ç®—å¯å‘å¼åˆ†æ•°
        factual_score = min(1.0, accuracy_indicators['factual_language'] / 2)
        uncertainty_penalty = min(0.3, accuracy_indicators['uncertainty_handling'] / 5)
        logic_score = min(1.0, accuracy_indicators['logical_connectors'] / 3)
        detail_score = min(1.0, accuracy_indicators['specific_details'] / 3)
        
        heuristic_score = max(0.0, (factual_score + logic_score + detail_score) / 3 - uncertainty_penalty)
        
        return EvaluationResult(
            metric_name=self.name,
            score=heuristic_score,
            max_score=1.0,
            details={
                'heuristic_indicators': accuracy_indicators,
                'factual_score': factual_score,
                'uncertainty_penalty': uncertainty_penalty,
                'logic_score': logic_score,
                'detail_score': detail_score
            },
            timestamp=datetime.now().isoformat()
        )
    
    def _calculate_semantic_similarity(self, response: str, target: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ç®€åŒ–çš„è¯­ä¹‰ç›¸ä¼¼æ€§è®¡ç®—ï¼šåŸºäºè¯æ±‡é‡å 
        response_words = set(response.lower().split())
        target_words = set(target.lower().split())
        
        intersection = len(response_words & target_words)
        union = len(response_words | target_words)
        
        if union == 0:
            return 0.0
        
        jaccard_similarity = intersection / union
        return jaccard_similarity

class ConsistencyMetric(MetricCalculator):
    """ä¸€è‡´æ€§æŒ‡æ ‡"""
    
    def __init__(self, weight: float = 1.0):
        super().__init__("consistency", weight)
    
    def calculate(self, response: str, expected: Optional[Dict[str, Any]] = None) -> EvaluationResult:
        """è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°"""
        consistency_scores = {}
        
        # æœ¯è¯­ä¸€è‡´æ€§
        terms_consistency = self._check_terminology_consistency(response)
        consistency_scores['terminology'] = terms_consistency
        
        # æ ¼å¼ä¸€è‡´æ€§
        format_consistency = self._check_format_consistency(response)
        consistency_scores['format'] = format_consistency
        
        # è¯­è°ƒä¸€è‡´æ€§
        tone_consistency = self._check_tone_consistency(response)
        consistency_scores['tone'] = tone_consistency
        
        # é€»è¾‘ä¸€è‡´æ€§
        logic_consistency = self._check_logic_consistency(response)
        consistency_scores['logic'] = logic_consistency
        
        # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§
        total_score = sum(consistency_scores.values()) / len(consistency_scores)
        
        return EvaluationResult(
            metric_name=self.name,
            score=total_score,
            max_score=1.0,
            details=consistency_scores,
            timestamp=datetime.now().isoformat()
        )
    
    def _check_terminology_consistency(self, response: str) -> float:
        """æ£€æŸ¥æœ¯è¯­ä¸€è‡´æ€§"""
        # æŸ¥æ‰¾å¯èƒ½çš„æœ¯è¯­å˜ä½“
        term_variants = {
            'user': ['ç”¨æˆ·', 'å®¢æˆ·', 'ä½¿ç”¨è€…'],
            'system': ['ç³»ç»Ÿ', 'å¹³å°', 'åº”ç”¨'],
            'data': ['æ•°æ®', 'ä¿¡æ¯', 'èµ„æ–™'],
            'process': ['æµç¨‹', 'è¿‡ç¨‹', 'æ­¥éª¤'],
            'result': ['ç»“æœ', 'è¾“å‡º', 'æˆæœ']
        }
        
        inconsistencies = 0
        total_checks = 0
        
        for category, variants in term_variants.items():
            found_variants = []
            for variant in variants:
                if variant in response:
                    found_variants.append(variant)
            
            if len(found_variants) > 1:
                inconsistencies += 1
            
            total_checks += 1
        
        consistency_score = 1.0 - (inconsistencies / total_checks) if total_checks > 0 else 1.0
        return consistency_score
    
    def _check_format_consistency(self, response: str) -> float:
        """æ£€æŸ¥æ ¼å¼ä¸€è‡´æ€§"""
        # æ£€æŸ¥åˆ—è¡¨æ ¼å¼ä¸€è‡´æ€§
        bullet_patterns = [r'[-â€¢*]\s+', r'\d+\.\s+', r'[a-zA-Z]\.\s+']
        used_patterns = []
        
        for pattern in bullet_patterns:
            if re.search(pattern, response):
                used_patterns.append(pattern)
        
        # æ£€æŸ¥æ ‡é¢˜æ ¼å¼ä¸€è‡´æ€§
        header_patterns = [r'#{1,6}\s+', r'ã€.*?ã€‘', r'\*\*.*?\*\*']
        used_headers = []
        
        for pattern in header_patterns:
            if re.search(pattern, response):
                used_headers.append(pattern)
        
        # æ ¼å¼ä¸€è‡´æ€§è¯„åˆ†
        format_score = 1.0
        if len(used_patterns) > 2:  # ä½¿ç”¨äº†è¿‡å¤šä¸åŒçš„åˆ—è¡¨æ ¼å¼
            format_score -= 0.3
        if len(used_headers) > 2:  # ä½¿ç”¨äº†è¿‡å¤šä¸åŒçš„æ ‡é¢˜æ ¼å¼
            format_score -= 0.3
        
        return max(0.0, format_score)
    
    def _check_tone_consistency(self, response: str) -> float:
        """æ£€æŸ¥è¯­è°ƒä¸€è‡´æ€§"""
        # æ£€æŸ¥æ­£å¼/éæ­£å¼è¯­è°ƒ
        formal_indicators = ['æ‚¨', 'è¯·', 'å»ºè®®', 'åˆ†æ', 'è¯„ä¼°']
        informal_indicators = ['ä½ ', 'å’±ä»¬', 'å¥½çš„', 'æ²¡é—®é¢˜']
        
        formal_count = sum(1 for indicator in formal_indicators if indicator in response)
        informal_count = sum(1 for indicator in informal_indicators if indicator in response)
        
        total_tone_indicators = formal_count + informal_count
        
        if total_tone_indicators == 0:
            return 1.0  # æ²¡æœ‰æ˜æ˜¾è¯­è°ƒæŒ‡ç¤ºå™¨
        
        # è®¡ç®—è¯­è°ƒä¸€è‡´æ€§
        dominant_tone_ratio = max(formal_count, informal_count) / total_tone_indicators
        return dominant_tone_ratio
    
    def _check_logic_consistency(self, response: str) -> float:
        """æ£€æŸ¥é€»è¾‘ä¸€è‡´æ€§"""
        # æ£€æŸ¥çŸ›ç›¾è¯æ±‡
        contradiction_patterns = [
            (r'å¿…é¡»', r'å¯é€‰'),
            (r'æ€»æ˜¯', r'æœ‰æ—¶'),
            (r'æ‰€æœ‰', r'éƒ¨åˆ†'),
            (r'ç»å¯¹', r'ç›¸å¯¹'),
            (r'æ°¸è¿œ', r'ä¸´æ—¶')
        ]
        
        contradictions = 0
        for positive, negative in contradiction_patterns:
            if re.search(positive, response, re.IGNORECASE) and re.search(negative, response, re.IGNORECASE):
                contradictions += 1
        
        # æ£€æŸ¥é€»è¾‘è¿æ¥è¯çš„ä½¿ç”¨
        logical_connectors = len(re.findall(r'(å› æ­¤|æ‰€ä»¥|ä½†æ˜¯|ç„¶è€Œ|è€Œä¸”|å¦å¤–)', response, re.IGNORECASE))
        
        # é€»è¾‘ä¸€è‡´æ€§è¯„åˆ†
        logic_score = 1.0 - (contradictions * 0.2)  # æ¯ä¸ªçŸ›ç›¾æ‰£0.2åˆ†
        
        # å¦‚æœæœ‰é€»è¾‘è¿æ¥è¯ï¼Œç¨å¾®æé«˜åˆ†æ•°
        if logical_connectors > 0:
            logic_score = min(1.0, logic_score + 0.1)
        
        return max(0.0, logic_score)

class PerformanceMetric(MetricCalculator):
    """æ€§èƒ½æŒ‡æ ‡"""
    
    def __init__(self, weight: float = 1.0):
        super().__init__("performance", weight)
    
    def calculate(self, response: str, expected: Optional[Dict[str, Any]] = None) -> EvaluationResult:
        """è®¡ç®—æ€§èƒ½åˆ†æ•°"""
        performance_scores = {}
        
        # å“åº”æ—¶é—´åˆ†æ•°ï¼ˆéœ€è¦å¤–éƒ¨æä¾›ï¼‰
        response_time = expected.get('response_time', 0) if expected else 0
        time_score = self._calculate_time_score(response_time)
        performance_scores['response_time'] = time_score
        
        # æ•ˆç‡åˆ†æ•°ï¼ˆå“åº”é•¿åº¦vså†…å®¹è´¨é‡ï¼‰
        efficiency_score = self._calculate_efficiency_score(response)
        performance_scores['efficiency'] = efficiency_score
        
        # èµ„æºä½¿ç”¨åˆ†æ•°ï¼ˆåŸºäºå“åº”å¤æ‚åº¦ï¼‰
        resource_score = self._calculate_resource_score(response)
        performance_scores['resource_usage'] = resource_score
        
        # è®¡ç®—æ€»ä½“æ€§èƒ½åˆ†æ•°
        total_score = sum(performance_scores.values()) / len(performance_scores)
        
        return EvaluationResult(
            metric_name=self.name,
            score=total_score,
            max_score=1.0,
            details=performance_scores,
            timestamp=datetime.now().isoformat()
        )
    
    def _calculate_time_score(self, response_time: float) -> float:
        """è®¡ç®—å“åº”æ—¶é—´åˆ†æ•°"""
        if response_time <= 1.0:
            return 1.0
        elif response_time <= 3.0:
            return 0.8
        elif response_time <= 5.0:
            return 0.6
        elif response_time <= 10.0:
            return 0.4
        else:
            return 0.2
    
    def _calculate_efficiency_score(self, response: str) -> float:
        """è®¡ç®—æ•ˆç‡åˆ†æ•°"""
        # åŸºäºä¿¡æ¯å¯†åº¦è®¡ç®—æ•ˆç‡
        word_count = len(response.split())
        char_count = len(response)
        
        # ä¿¡æ¯å¯†åº¦æŒ‡æ ‡
        info_density = word_count / max(1, char_count / 5)  # æ¯5ä¸ªå­—ç¬¦çš„è¯æ±‡æ•°
        
        # æ ‡å‡†åŒ–æ•ˆç‡åˆ†æ•°
        efficiency_score = min(1.0, info_density)
        
        return efficiency_score
    
    def _calculate_resource_score(self, response: str) -> float:
        """è®¡ç®—èµ„æºä½¿ç”¨åˆ†æ•°"""
        # åŸºäºå“åº”å¤æ‚åº¦ä¼°ç®—èµ„æºä½¿ç”¨
        complexity_indicators = {
            'length': len(response),
            'vocabulary_diversity': len(set(response.split())),
            'structure_complexity': len(re.findall(r'[.!?]', response)),
            'formatting_elements': len(re.findall(r'[*#\-\d\.]', response))
        }
        
        # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
        complexity_score = (
            min(1.0, complexity_indicators['length'] / 1000) * 0.3 +
            min(1.0, complexity_indicators['vocabulary_diversity'] / 200) * 0.3 +
            min(1.0, complexity_indicators['structure_complexity'] / 10) * 0.2 +
            min(1.0, complexity_indicators['formatting_elements'] / 20) * 0.2
        )
        
        # èµ„æºåˆ†æ•°ä¸å¤æ‚åº¦æˆåæ¯”ï¼ˆå¤æ‚åº¦è¶Šé«˜ï¼Œèµ„æºä½¿ç”¨è¶Šå¤šï¼Œåˆ†æ•°è¶Šä½ï¼‰
        resource_score = max(0.2, 1.0 - complexity_score * 0.5)
        
        return resource_score

class ComprehensiveEvaluator:
    """ç»¼åˆè¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics = [
            ContentQualityMetric(weight=0.3),
            StructureMetric(weight=0.2),
            AccuracyMetric(weight=0.25),
            ConsistencyMetric(weight=0.15),
            PerformanceMetric(weight=0.1)
        ]
    
    def evaluate(self, response: str, expected: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ‰§è¡Œç»¼åˆè¯„ä¼°"""
        results = {}
        total_weighted_score = 0
        total_weight = 0
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        for metric in self.metrics:
            result = metric.calculate(response, expected)
            results[metric.name] = result
            total_weighted_score += result.score * metric.weight
            total_weight += metric.weight
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        overall_score = total_weighted_score / total_weight if total_weight > 0 else 0
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        evaluation_report = {
            'overall_score': overall_score,
            'grade': self._calculate_grade(overall_score),
            'metric_scores': {name: result.score for name, result in results.items()},
            'detailed_results': {name: result.details for name, result in results.items()},
            'recommendations': self._generate_recommendations(results),
            'timestamp': datetime.now().isoformat()
        }
        
        return evaluation_report
    
    def _calculate_grade(self, score: float) -> str:
        """è®¡ç®—ç­‰çº§"""
        if score >= 0.9:
            return 'A+'
        elif score >= 0.85:
            return 'A'
        elif score >= 0.8:
            return 'A-'
        elif score >= 0.75:
            return 'B+'
        elif score >= 0.7:
            return 'B'
        elif score >= 0.65:
            return 'B-'
        elif score >= 0.6:
            return 'C+'
        elif score >= 0.55:
            return 'C'
        elif score >= 0.5:
            return 'C-'
        else:
            return 'D'
    
    def _generate_recommendations(self, results: Dict[str, EvaluationResult]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åˆ†æå„é¡¹æŒ‡æ ‡ï¼Œç”Ÿæˆå…·ä½“å»ºè®®
        for metric_name, result in results.items():
            if result.score < 0.7:
                if metric_name == 'content_quality':
                    recommendations.append("å»ºè®®å¢å¼ºå†…å®¹æ·±åº¦å’Œå‡†ç¡®æ€§ï¼Œæä¾›æ›´å¤šå…·ä½“ç»†èŠ‚")
                elif metric_name == 'structure':
                    recommendations.append("å»ºè®®æ”¹å–„å†…å®¹ç»“æ„ï¼Œä½¿ç”¨æ›´å¤šæ ‡é¢˜ã€åˆ—è¡¨å’Œæ ¼å¼åŒ–å…ƒç´ ")
                elif metric_name == 'accuracy':
                    recommendations.append("å»ºè®®æé«˜å†…å®¹å‡†ç¡®æ€§ï¼Œå¢åŠ äº‹å®ä¾æ®å’Œé€»è¾‘è¿æ¥")
                elif metric_name == 'consistency':
                    recommendations.append("å»ºè®®ä¿æŒæœ¯è¯­å’Œæ ¼å¼çš„ä¸€è‡´æ€§ï¼Œé¿å…çŸ›ç›¾è¡¨è¿°")
                elif metric_name == 'performance':
                    recommendations.append("å»ºè®®ä¼˜åŒ–å“åº”æ•ˆç‡ï¼Œå¹³è¡¡å†…å®¹è´¨é‡å’Œå“åº”é€Ÿåº¦")
        
        if not recommendations:
            recommendations.append("æ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰è´¨é‡æ°´å¹³")
        
        return recommendations

# æ‰¹é‡è¯„ä¼°å·¥å…·
class BatchEvaluator:
    """æ‰¹é‡è¯„ä¼°å·¥å…·"""
    
    def __init__(self):
        self.evaluator = ComprehensiveEvaluator()
    
    def evaluate_batch(self, responses: List[str], expected_list: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """æ‰¹é‡è¯„ä¼°å¤šä¸ªå“åº”"""
        if expected_list is None:
            expected_list = [None] * len(responses)
        
        results = []
        for i, (response, expected) in enumerate(zip(responses, expected_list)):
            result = self.evaluator.evaluate(response, expected)
            result['response_id'] = i
            results.append(result)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        scores = [r['overall_score'] for r in results]
        statistics_report = {
            'total_responses': len(responses),
            'average_score': statistics.mean(scores),
            'median_score': statistics.median(scores),
            'std_deviation': statistics.stdev(scores) if len(scores) > 1 else 0,
            'min_score': min(scores),
            'max_score': max(scores),
            'grade_distribution': self._calculate_grade_distribution(scores),
            'results': results
        }
        
        return statistics_report
    
    def _calculate_grade_distribution(self, scores: List[float]) -> Dict[str, int]:
        """è®¡ç®—ç­‰çº§åˆ†å¸ƒ"""
        grade_counts = {}
        for score in scores:
            grade = self.evaluator._calculate_grade(score)
            grade_counts[grade] = grade_counts.get(grade, 0) + 1
        
        return grade_counts

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    evaluator = ComprehensiveEvaluator()
    
    # æµ‹è¯•å“åº”
    test_response = """
    ğŸ¯ éœ€æ±‚ç†è§£
    æ ¹æ®æ‚¨çš„é—®é¢˜ï¼Œæˆ‘éœ€è¦ä¸ºæ‚¨æä¾›è¥é”€ç­–ç•¥å»ºè®®ã€‚
    
    ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ
    - å¸‚åœºç«äº‰æ¿€çƒˆï¼Œéœ€è¦å·®å¼‚åŒ–å®šä½
    - ç›®æ ‡å®¢æˆ·ç¾¤ä½“éœ€è¦ç²¾å‡†è¯†åˆ«
    - æ•°å­—åŒ–è¥é”€æ¸ é“é‡è¦æ€§æ—¥ç›Šå¢åŠ 
    
    ğŸš€ è§£å†³æ–¹æ¡ˆ
    1. åˆ¶å®šå“ç‰Œå·®å¼‚åŒ–ç­–ç•¥
    2. å»ºç«‹ç²¾å‡†å®¢æˆ·ç”»åƒ
    3. æ•´åˆå¤šæ¸ é“è¥é”€æ–¹æ¡ˆ
    4. å»ºç«‹æ•°æ®é©±åŠ¨çš„å†³ç­–æœºåˆ¶
    
    ğŸ“Š è¡¥å……ä¿¡æ¯
    å»ºè®®å®šæœŸè¯„ä¼°è¥é”€æ•ˆæœï¼Œæ ¹æ®æ•°æ®åé¦ˆè°ƒæ•´ç­–ç•¥ã€‚
    """
    
    # æœŸæœ›æ ‡å‡†
    expected = {
        'keywords': ['è¥é”€', 'ç­–ç•¥', 'å®¢æˆ·', 'æ•°æ®'],
        'patterns': ['ğŸ¯', 'ğŸ’¡', 'ğŸš€', 'ğŸ“Š'],
        'format': r'ğŸ¯.*ğŸ’¡.*ğŸš€.*ğŸ“Š'
    }
    
    # æ‰§è¡Œè¯„ä¼°
    result = evaluator.evaluate(test_response, expected)
    
    print(f"ç»¼åˆè¯„åˆ†: {result['overall_score']:.3f}")
    print(f"ç­‰çº§: {result['grade']}")
    print(f"å„é¡¹æŒ‡æ ‡: {result['metric_scores']}")
    print(f"æ”¹è¿›å»ºè®®: {result['recommendations']}")
    
    # æ‰¹é‡è¯„ä¼°ç¤ºä¾‹
    batch_evaluator = BatchEvaluator()
    responses = [test_response, "è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å›ç­”", "è¯¦ç»†çš„ä¸“ä¸šåˆ†æå†…å®¹..."]
    batch_result = batch_evaluator.evaluate_batch(responses)
    
    print(f"\næ‰¹é‡è¯„ä¼°ç»“æœ:")
    print(f"å¹³å‡åˆ†: {batch_result['average_score']:.3f}")
    print(f"ç­‰çº§åˆ†å¸ƒ: {batch_result['grade_distribution']}")