#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§ª ç»¼åˆæç¤ºè¯æµ‹è¯•æ¡†æ¶ (Comprehensive Prompt Testing Framework)
ç”¨äºè‡ªåŠ¨åŒ–æµ‹è¯•æç¤ºè¯æ€§èƒ½ã€è´¨é‡å’Œä¸€è‡´æ€§çš„å®Œæ•´æµ‹è¯•å¥—ä»¶
"""

import json
import re
import time
from typing import Dict, List, Any, Optional
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime
import statistics
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TestCase:
    """æµ‹è¯•ç”¨ä¾‹æ•°æ®ç»“æ„"""
    id: str
    name: str
    input_text: str
    expected_patterns: List[str]
    expected_format: str
    category: str
    priority: str
    metadata: Dict[str, Any]

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç»“æ„"""
    test_id: str
    passed: bool
    score: float
    response_time: float
    output_text: str
    pattern_matches: Dict[str, bool]
    format_compliance: bool
    error_messages: List[str]
    timestamp: str

class PromptTestingFramework:
    """æç¤ºè¯æµ‹è¯•æ¡†æ¶ä¸»ç±»"""
    
    def __init__(self, config_path: str = "test_config.json"):
        self.config = self._load_config(config_path)
        self.test_cases: List[TestCase] = []
        self.results: List[TestResult] = []
        self.evaluation_metrics = {
            'accuracy': 0.0,
            'consistency': 0.0,
            'response_time': 0.0,
            'format_compliance': 0.0,
            'pattern_coverage': 0.0
        }
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½æµ‹è¯•é…ç½®"""
        default_config = {
            "test_timeout": 30,
            "retry_attempts": 3,
            "min_score_threshold": 0.7,
            "evaluation_weights": {
                "pattern_match": 0.3,
                "format_compliance": 0.2,
                "response_time": 0.1,
                "content_quality": 0.4
            },
            "output_formats": {
                "ssap_4_section": r"ğŸ¯.*ğŸ’¡.*ğŸš€.*ğŸ“Š",
                "workflow_format": r"<workflow>.*FUNCTION.*BEGIN.*END.*</workflow>",
                "structured_json": r"^\{.*\}$"
            }
        }
        
        try:
            if Path(config_path).exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    return {**default_config, **config}
        except Exception as e:
            logger.warning(f"é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config
    
    def add_test_case(self, test_case: TestCase) -> None:
        """æ·»åŠ æµ‹è¯•ç”¨ä¾‹"""
        self.test_cases.append(test_case)
        logger.info(f"å·²æ·»åŠ æµ‹è¯•ç”¨ä¾‹: {test_case.name}")
    
    def load_test_cases_from_json(self, file_path: str) -> None:
        """ä»JSONæ–‡ä»¶åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            for case_data in data.get('test_cases', []):
                test_case = TestCase(
                    id=case_data['id'],
                    name=case_data['name'],
                    input_text=case_data['input_text'],
                    expected_patterns=case_data.get('expected_patterns', []),
                    expected_format=case_data.get('expected_format', ''),
                    category=case_data.get('category', 'general'),
                    priority=case_data.get('priority', 'medium'),
                    metadata=case_data.get('metadata', {})
                )
                self.add_test_case(test_case)
                
        except Exception as e:
            logger.error(f"åŠ è½½æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {e}")
    
    def run_single_test(self, test_case: TestCase, ai_response_func) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        logger.info(f"æ‰§è¡Œæµ‹è¯•: {test_case.name}")
        
        start_time = time.time()
        error_messages = []
        
        try:
            # è·å–AIå“åº”
            response = ai_response_func(test_case.input_text)
            response_time = time.time() - start_time
            
            # æ¨¡å¼åŒ¹é…æ£€æŸ¥
            pattern_matches = {}
            for pattern in test_case.expected_patterns:
                matches = bool(re.search(pattern, response, re.DOTALL | re.IGNORECASE))
                pattern_matches[pattern] = matches
            
            # æ ¼å¼åˆè§„æ€§æ£€æŸ¥
            format_compliance = self._check_format_compliance(
                response, test_case.expected_format
            )
            
            # è®¡ç®—åˆ†æ•°
            score = self._calculate_score(
                pattern_matches, format_compliance, response_time, response
            )
            
            # åˆ¤æ–­æ˜¯å¦é€šè¿‡
            passed = score >= self.config['min_score_threshold']
            
            result = TestResult(
                test_id=test_case.id,
                passed=passed,
                score=score,
                response_time=response_time,
                output_text=response,
                pattern_matches=pattern_matches,
                format_compliance=format_compliance,
                error_messages=error_messages,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            error_messages.append(str(e))
            result = TestResult(
                test_id=test_case.id,
                passed=False,
                score=0.0,
                response_time=time.time() - start_time,
                output_text="",
                pattern_matches={},
                format_compliance=False,
                error_messages=error_messages,
                timestamp=datetime.now().isoformat()
            )
        
        self.results.append(result)
        return result
    
    def _check_format_compliance(self, response: str, expected_format: str) -> bool:
        """æ£€æŸ¥æ ¼å¼åˆè§„æ€§"""
        if not expected_format:
            return True
        
        # æ£€æŸ¥é¢„å®šä¹‰æ ¼å¼
        if expected_format in self.config['output_formats']:
            pattern = self.config['output_formats'][expected_format]
            return bool(re.search(pattern, response, re.DOTALL | re.IGNORECASE))
        
        # è‡ªå®šä¹‰æ ¼å¼æ£€æŸ¥
        return bool(re.search(expected_format, response, re.DOTALL | re.IGNORECASE))
    
    def _calculate_score(self, pattern_matches: Dict[str, bool], 
                        format_compliance: bool, response_time: float, 
                        response: str) -> float:
        """è®¡ç®—ç»¼åˆåˆ†æ•°"""
        weights = self.config['evaluation_weights']
        
        # æ¨¡å¼åŒ¹é…åˆ†æ•°
        pattern_score = sum(pattern_matches.values()) / len(pattern_matches) if pattern_matches else 0
        
        # æ ¼å¼åˆè§„åˆ†æ•°
        format_score = 1.0 if format_compliance else 0.0
        
        # å“åº”æ—¶é—´åˆ†æ•° (è¶Šå¿«è¶Šå¥½ï¼Œä½†è®¾ç½®åˆç†ä¸Šé™)
        time_score = max(0, 1.0 - (response_time / 10))
        
        # å†…å®¹è´¨é‡åˆ†æ•° (åŸºäºé•¿åº¦å’Œå¤æ‚åº¦çš„ç®€å•è¯„ä¼°)
        content_score = min(1.0, len(response) / 500) if response else 0
        
        # åŠ æƒè®¡ç®—æœ€ç»ˆåˆ†æ•°
        final_score = (
            pattern_score * weights['pattern_match'] +
            format_score * weights['format_compliance'] +
            time_score * weights['response_time'] +
            content_score * weights['content_quality']
        )
        
        return round(final_score, 3)
    
    def run_all_tests(self, ai_response_func) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹"""
        logger.info(f"å¼€å§‹è¿è¡Œ {len(self.test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        self.results = []
        passed_tests = 0
        
        for test_case in self.test_cases:
            result = self.run_single_test(test_case, ai_response_func)
            if result.passed:
                passed_tests += 1
        
        # è®¡ç®—ç»¼åˆæŒ‡æ ‡
        self._calculate_metrics()
        
        summary = {
            'total_tests': len(self.test_cases),
            'passed_tests': passed_tests,
            'failed_tests': len(self.test_cases) - passed_tests,
            'success_rate': passed_tests / len(self.test_cases) if self.test_cases else 0,
            'average_score': statistics.mean([r.score for r in self.results]) if self.results else 0,
            'average_response_time': statistics.mean([r.response_time for r in self.results]) if self.results else 0,
            'metrics': self.evaluation_metrics
        }
        
        logger.info(f"æµ‹è¯•å®Œæˆ - æˆåŠŸç‡: {summary['success_rate']:.2%}")
        return summary
    
    def _calculate_metrics(self) -> None:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if not self.results:
            return
        
        # å‡†ç¡®ç‡ (é€šè¿‡çš„æµ‹è¯•æ¯”ä¾‹)
        self.evaluation_metrics['accuracy'] = sum(1 for r in self.results if r.passed) / len(self.results)
        
        # ä¸€è‡´æ€§ (åˆ†æ•°çš„æ ‡å‡†å·®ï¼Œè¶Šå°è¶Šä¸€è‡´)
        scores = [r.score for r in self.results]
        self.evaluation_metrics['consistency'] = 1.0 - (statistics.stdev(scores) if len(scores) > 1 else 0)
        
        # å¹³å‡å“åº”æ—¶é—´
        self.evaluation_metrics['response_time'] = statistics.mean([r.response_time for r in self.results])
        
        # æ ¼å¼åˆè§„ç‡
        self.evaluation_metrics['format_compliance'] = sum(1 for r in self.results if r.format_compliance) / len(self.results)
        
        # æ¨¡å¼è¦†ç›–ç‡
        total_patterns = sum(len(r.pattern_matches) for r in self.results)
        matched_patterns = sum(sum(r.pattern_matches.values()) for r in self.results)
        self.evaluation_metrics['pattern_coverage'] = matched_patterns / total_patterns if total_patterns > 0 else 0
    
    def generate_report(self, output_path: Optional[str] = None) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = {
            'test_summary': {
                'timestamp': datetime.now().isoformat(),
                'total_tests': len(self.test_cases),
                'passed_tests': sum(1 for r in self.results if r.passed),
                'failed_tests': sum(1 for r in self.results if not r.passed),
                'success_rate': self.evaluation_metrics['accuracy'],
                'average_score': statistics.mean([r.score for r in self.results]) if self.results else 0
            },
            'evaluation_metrics': self.evaluation_metrics,
            'test_results': [asdict(result) for result in self.results],
            'failed_tests': [
                {
                    'test_id': r.test_id,
                    'score': r.score,
                    'errors': r.error_messages,
                    'missing_patterns': [p for p, matched in r.pattern_matches.items() if not matched]
                }
                for r in self.results if not r.passed
            ]
        }
        
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            logger.info(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        return json.dumps(report, ensure_ascii=False, indent=2)
    
    def create_test_suite_from_dataset(self, dataset_path: str) -> None:
        """ä»æ•°æ®é›†åˆ›å»ºæµ‹è¯•å¥—ä»¶"""
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
            
            for idx, sample in enumerate(dataset.get('train', [])[:10]):  # å–å‰10ä¸ªæ ·æœ¬
                test_case = TestCase(
                    id=f"dataset_{idx}",
                    name=f"Dataset Sample {idx}",
                    input_text=sample['instruction'],
                    expected_patterns=['FUNCTION', 'BEGIN', 'END'] if 'workflow' in sample['output'] else [],
                    expected_format='workflow_format' if 'workflow' in sample['output'] else '',
                    category=sample.get('metadata', {}).get('category', 'general'),
                    priority='high',
                    metadata=sample.get('metadata', {})
                )
                self.add_test_case(test_case)
                
        except Exception as e:
            logger.error(f"ä»æ•°æ®é›†åˆ›å»ºæµ‹è¯•å¥—ä»¶å¤±è´¥: {e}")

# ç¤ºä¾‹AIå“åº”å‡½æ•° (éœ€è¦æ ¹æ®å®é™…AIæ¥å£è°ƒæ•´)
def mock_ai_response(prompt: str) -> str:
    """æ¨¡æ‹ŸAIå“åº”å‡½æ•°"""
    if "å·¥ä½œæµ" in prompt:
        return """<workflow>
FUNCTION ç¤ºä¾‹å·¥ä½œæµ(è¾“å…¥å‚æ•°):
BEGIN
  è¿ç”¨[Knowledge.ä¸“ä¸šçŸ¥è¯†]åˆ†æé—®é¢˜
  é€šè¿‡[Skills.åˆ†ææŠ€èƒ½]å¤„ç†ä»»åŠ¡
  ä½¿ç”¨[Format.è¾“å‡ºæ ¼å¼]ç”Ÿæˆç»“æœ
  RETURN å¤„ç†ç»“æœ
END
</workflow>"""
    else:
        return "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„AIå“åº”ã€‚"

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    framework = PromptTestingFramework()
    
    # æ·»åŠ æµ‹è¯•ç”¨ä¾‹
    test_case = TestCase(
        id="test_001",
        name="å·¥ä½œæµç”Ÿæˆæµ‹è¯•",
        input_text="è®¾è®¡ä¸€ä¸ªæ•°æ®å¤„ç†å·¥ä½œæµ",
        expected_patterns=["FUNCTION", "BEGIN", "END", "workflow"],
        expected_format="workflow_format",
        category="workflow",
        priority="high",
        metadata={"domain": "data_processing"}
    )
    framework.add_test_case(test_case)
    
    # è¿è¡Œæµ‹è¯•
    summary = framework.run_all_tests(mock_ai_response)
    print(f"æµ‹è¯•å®Œæˆ: {summary}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = framework.generate_report("test_report.json")
    print("æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ")