#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ AIæç¤ºè¯æµ‹è¯•æ¡†æ¶ - Gradio Webç•Œé¢
æä¾›ç”¨æˆ·å‹å¥½çš„Webç•Œé¢è¿›è¡ŒAIæ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°
"""

import gradio as gr
import json
import sys
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
import logging
import traceback
import asyncio
import threading
import time

# æ·»åŠ æ ¸å¿ƒæ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "core"))

from core.prompt_testing_framework import PromptTestingFramework, TestCase
from core.evaluation_metrics import ComprehensiveEvaluator
from core.openrouter_integration import OpenRouterClient, MultiModelTester, AIEvaluator

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GradioTestInterface:
    """Gradioæµ‹è¯•ç•Œé¢ç±»"""
    
    def __init__(self):
        self.client = None
        self.framework = PromptTestingFramework()
        self.evaluator = ComprehensiveEvaluator()
        self.multi_model_tester = None
        self.ai_evaluator = None
        self.available_models = []
        self.test_cases = []
        
        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        self.load_test_cases()
        
        # å°è¯•åˆå§‹åŒ–OpenRouterå®¢æˆ·ç«¯
        self.initialize_openrouter()
    
    def initialize_openrouter(self):
        """åˆå§‹åŒ–OpenRouterå®¢æˆ·ç«¯"""
        try:
            self.client = OpenRouterClient("config/openrouter_config.json")
            self.multi_model_tester = MultiModelTester(self.client)
            self.ai_evaluator = AIEvaluator(self.client)
            self.available_models = self.client.list_available_models()
            logger.info("OpenRouterå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"OpenRouterå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def load_test_cases(self):
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        try:
            self.framework.load_test_cases_from_json("examples/example_test_cases.json")
            self.test_cases = self.framework.test_cases
            logger.info(f"åŠ è½½äº† {len(self.test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        except Exception as e:
            logger.error(f"åŠ è½½æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {e}")
            self.test_cases = []
    
    def get_model_choices(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        if self.available_models:
            return [f"{model.id} - {model.name}" for model in self.available_models]
        else:
            return ["deepseek/deepseek-r1-0528:free - DeepSeek R1 (å…è´¹)"]
    
    def get_test_case_choices(self) -> List[str]:
        """è·å–æµ‹è¯•ç”¨ä¾‹é€‰æ‹©"""
        if self.test_cases:
            return [f"{tc.id} - {tc.name}" for tc in self.test_cases]
        else:
            return ["æ— å¯ç”¨æµ‹è¯•ç”¨ä¾‹"]
    
    def parse_model_selection(self, model_choice: str) -> str:
        """è§£ææ¨¡å‹é€‰æ‹©"""
        if " - " in model_choice:
            return model_choice.split(" - ")[0]
        return model_choice
    
    def parse_test_case_selection(self, test_choice: str) -> str:
        """è§£ææµ‹è¯•ç”¨ä¾‹é€‰æ‹©"""
        if " - " in test_choice:
            return test_choice.split(" - ")[0]
        return test_choice
    
    def run_single_test(self, 
                       model_choice: str, 
                       test_choice: str, 
                       custom_prompt: str = "") -> Tuple[str, str, str]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        try:
            if not self.client:
                return "âŒ é”™è¯¯", "OpenRouterå®¢æˆ·ç«¯æœªåˆå§‹åŒ–", ""
            
            # è§£ææ¨¡å‹é€‰æ‹©
            model_id = self.parse_model_selection(model_choice)
            
            # è·å–æµ‹è¯•ç”¨ä¾‹
            if custom_prompt.strip():
                # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯
                test_case = TestCase(
                    id="custom_test",
                    name="è‡ªå®šä¹‰æµ‹è¯•",
                    input_text=custom_prompt,
                    expected_patterns=[],
                    expected_format="",
                    category="custom",
                    priority="medium",
                    metadata={"type": "custom"}
                )
            else:
                # ä½¿ç”¨é¢„å®šä¹‰æµ‹è¯•ç”¨ä¾‹
                test_case_id = self.parse_test_case_selection(test_choice)
                test_case = next((tc for tc in self.test_cases if tc.id == test_case_id), None)
                
                if not test_case:
                    return "âŒ é”™è¯¯", "æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹", ""
            
            # åˆ›å»ºAIå“åº”å‡½æ•°
            def ai_response_func(prompt: str) -> str:
                return self.client.generate_response(model_id, prompt)
            
            # è¿è¡Œæµ‹è¯•
            start_time = datetime.now()
            result = self.framework.run_single_test(test_case, ai_response_func)
            end_time = datetime.now()
            
            # è·å–æˆæœ¬ä¿¡æ¯
            cost_summary = self.client.get_cost_summary()
            
            # æ ¼å¼åŒ–ç»“æœ
            status = "âœ… é€šè¿‡" if result.passed else "âŒ å¤±è´¥"
            
            details = f"""
ğŸ“Š **æµ‹è¯•ç»“æœè¯¦æƒ…**

**åŸºæœ¬ä¿¡æ¯:**
- æµ‹è¯•ç”¨ä¾‹: {test_case.name}
- æ¨¡å‹: {model_choice}
- æµ‹è¯•æ—¶é—´: {(end_time - start_time).total_seconds():.2f}ç§’

**è¯„ä¼°ç»“æœ:**
- é€šè¿‡çŠ¶æ€: {status}
- ç»¼åˆå¾—åˆ†: {result.score:.3f}
- å“åº”æ—¶é—´: {result.response_time:.2f}ç§’
- æ ¼å¼åˆè§„: {'âœ… æ˜¯' if result.format_compliance else 'âŒ å¦'}

**æ¨¡å¼åŒ¹é…:**
- åŒ¹é…é¡¹: {result.pattern_matches}
- æœŸæœ›æ¨¡å¼: {test_case.expected_patterns}

**æˆæœ¬ä¿¡æ¯:**
- æœ¬æ¬¡æµ‹è¯•æˆæœ¬: ${cost_summary.get('total_cost', 0):.6f}
- æ€»è¯·æ±‚æ•°: {cost_summary.get('requests_count', 0)}

**é”™è¯¯ä¿¡æ¯:**
{result.error_messages if result.error_messages else 'æ— é”™è¯¯'}
"""
            
            return status, details, result.output_text
            
        except Exception as e:
            logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return "âŒ é”™è¯¯", f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}", ""
    
    def run_model_comparison(self, 
                           model_choices: List[str], 
                           test_choice: str,
                           custom_prompt: str = "") -> Tuple[str, str]:
        """è¿è¡Œæ¨¡å‹æ¯”è¾ƒ"""
        try:
            if not self.client:
                return "âŒ é”™è¯¯", "OpenRouterå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
            
            if len(model_choices) < 2:
                return "âŒ é”™è¯¯", "è¯·è‡³å°‘é€‰æ‹©ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ"
            
            # è§£ææ¨¡å‹é€‰æ‹©
            model_ids = [self.parse_model_selection(choice) for choice in model_choices]
            
            # è·å–æµ‹è¯•ç”¨ä¾‹
            if custom_prompt.strip():
                prompt = custom_prompt
            else:
                test_case_id = self.parse_test_case_selection(test_choice)
                test_case = next((tc for tc in self.test_cases if tc.id == test_case_id), None)
                
                if not test_case:
                    return "âŒ é”™è¯¯", "æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹"
                
                prompt = test_case.input_text
            
            # è¿è¡Œæ¯”è¾ƒæµ‹è¯•
            comparison_results = {}
            for model_id in model_ids:
                try:
                    start_time = time.time()
                    response = self.client.generate_response(model_id, prompt)
                    end_time = time.time()
                    
                    comparison_results[model_id] = {
                        'success': True,
                        'response': response,
                        'response_time': end_time - start_time,
                        'response_length': len(response)
                    }
                except Exception as e:
                    comparison_results[model_id] = {
                        'success': False,
                        'error': str(e),
                        'response': '',
                        'response_time': 0,
                        'response_length': 0
                    }
            
            # è·å–æˆæœ¬ä¿¡æ¯
            cost_summary = self.client.get_cost_summary()
            
            # æ ¼å¼åŒ–æ¯”è¾ƒç»“æœ
            comparison_text = "ğŸ” **æ¨¡å‹æ¯”è¾ƒç»“æœ**\n\n"
            
            for i, (model_id, result) in enumerate(comparison_results.items(), 1):
                model_name = next((choice for choice in model_choices if model_id in choice), model_id)
                
                comparison_text += f"## {i}. {model_name}\n\n"
                
                if result['success']:
                    comparison_text += f"**çŠ¶æ€:** âœ… æˆåŠŸ\n"
                    comparison_text += f"**å“åº”æ—¶é—´:** {result['response_time']:.2f}ç§’\n"
                    comparison_text += f"**å“åº”é•¿åº¦:** {result['response_length']}å­—ç¬¦\n\n"
                    comparison_text += f"**å“åº”å†…å®¹:**\n{result['response'][:500]}{'...' if len(result['response']) > 500 else ''}\n\n"
                else:
                    comparison_text += f"**çŠ¶æ€:** âŒ å¤±è´¥\n"
                    comparison_text += f"**é”™è¯¯:** {result['error']}\n\n"
                
                comparison_text += "---\n\n"
            
            # æ·»åŠ æˆæœ¬ä¿¡æ¯
            comparison_text += f"""
ğŸ’° **æˆæœ¬æ‘˜è¦**
- æ€»æˆæœ¬: ${cost_summary.get('total_cost', 0):.6f}
- è¯·æ±‚æ•°: {cost_summary.get('requests_count', 0)}
- å¹³å‡æˆæœ¬: ${cost_summary.get('average_cost_per_request', 0):.6f}
"""
            
            return "âœ… å®Œæˆ", comparison_text
            
        except Exception as e:
            logger.error(f"æ¨¡å‹æ¯”è¾ƒå¤±è´¥: {e}")
            return "âŒ é”™è¯¯", f"æ¨¡å‹æ¯”è¾ƒå¤±è´¥: {str(e)}"
    
    def run_batch_test(self, 
                      model_choice: str, 
                      test_suite: str) -> Tuple[str, str]:
        """è¿è¡Œæ‰¹é‡æµ‹è¯•"""
        try:
            if not self.client:
                return "âŒ é”™è¯¯", "OpenRouterå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
            
            # è§£ææ¨¡å‹é€‰æ‹©
            model_id = self.parse_model_selection(model_choice)
            
            # é€‰æ‹©æµ‹è¯•ç”¨ä¾‹
            if test_suite == "basic":
                selected_tests = [tc for tc in self.test_cases if tc.category in ["ssap", "workflow", "basic"]][:5]
            elif test_suite == "comprehensive":
                selected_tests = self.test_cases
            elif test_suite == "ssap":
                selected_tests = [tc for tc in self.test_cases if "ssap" in tc.id.lower()]
            elif test_suite == "workflow":
                selected_tests = [tc for tc in self.test_cases if "workflow" in tc.id.lower()]
            else:
                selected_tests = self.test_cases[:10]  # é»˜è®¤å‰10ä¸ª
            
            if not selected_tests:
                return "âŒ é”™è¯¯", "æœªæ‰¾åˆ°åŒ¹é…çš„æµ‹è¯•ç”¨ä¾‹"
            
            # åˆ›å»ºAIå“åº”å‡½æ•°
            def ai_response_func(prompt: str) -> str:
                return self.client.generate_response(model_id, prompt)
            
            # è¿è¡Œæ‰¹é‡æµ‹è¯•
            results = []
            for i, test_case in enumerate(selected_tests, 1):
                try:
                    result = self.framework.run_single_test(test_case, ai_response_func)
                    results.append({
                        'test_case': test_case,
                        'result': result,
                        'index': i
                    })
                except Exception as e:
                    logger.error(f"æµ‹è¯• {test_case.name} å¤±è´¥: {e}")
                    continue
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_tests = len(results)
            passed_tests = sum(1 for r in results if r['result'].passed)
            avg_score = sum(r['result'].score for r in results) / total_tests if total_tests > 0 else 0
            avg_time = sum(r['result'].response_time for r in results) / total_tests if total_tests > 0 else 0
            
            # è·å–æˆæœ¬ä¿¡æ¯
            cost_summary = self.client.get_cost_summary()
            
            # æ ¼å¼åŒ–ç»“æœ
            batch_text = f"""
ğŸ“¦ **æ‰¹é‡æµ‹è¯•ç»“æœ**

**æ¦‚è¦ç»Ÿè®¡:**
- æµ‹è¯•å¥—ä»¶: {test_suite}
- æ¨¡å‹: {model_choice}
- æ€»æµ‹è¯•æ•°: {total_tests}
- é€šè¿‡æµ‹è¯•: {passed_tests}
- å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}
- æˆåŠŸç‡: {passed_tests/total_tests:.1%}
- å¹³å‡åˆ†æ•°: {avg_score:.3f}
- å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’

**è¯¦ç»†ç»“æœ:**
"""
            
            for r in results:
                status = "âœ… é€šè¿‡" if r['result'].passed else "âŒ å¤±è´¥"
                batch_text += f"\n{r['index']}. {r['test_case'].name} - {status} (åˆ†æ•°: {r['result'].score:.3f})"
            
            batch_text += f"""

ğŸ’° **æˆæœ¬ä¿¡æ¯:**
- æ€»æˆæœ¬: ${cost_summary.get('total_cost', 0):.6f}
- è¯·æ±‚æ•°: {cost_summary.get('requests_count', 0)}
- å¹³å‡æˆæœ¬: ${cost_summary.get('average_cost_per_request', 0):.6f}
"""
            
            return "âœ… å®Œæˆ", batch_text
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æµ‹è¯•å¤±è´¥: {e}")
            return "âŒ é”™è¯¯", f"æ‰¹é‡æµ‹è¯•å¤±è´¥: {str(e)}"
    
    def get_model_info(self, model_choice: str) -> str:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        try:
            model_id = self.parse_model_selection(model_choice)
            
            if self.available_models:
                model = next((m for m in self.available_models if m.id == model_id), None)
                if model:
                    return f"""
ğŸ“‹ **æ¨¡å‹ä¿¡æ¯**

**åç§°:** {model.name}
**ID:** {model.id}
**æè¿°:** {model.description}
**æˆæœ¬:** ${model.cost_per_token:.8f}/token
**æ¨èç”¨é€”:** {', '.join(model.recommended_for)}
**ä¸Šä¸‹æ–‡é™åˆ¶:** {getattr(model, 'context_limit', 'N/A')}
"""
            
            return f"æ¨¡å‹ID: {model_id}"
            
        except Exception as e:
            return f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {str(e)}"
    
    def create_interface(self):
        """åˆ›å»ºGradioç•Œé¢"""
        
        # è‡ªå®šä¹‰CSSæ ·å¼
        custom_css = """
        .gradio-container {
            max-width: 1200px !important;
        }
        .tab-nav {
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%) !important;
        }
        .tab-nav button {
            color: white !important;
        }
        .tab-nav button.selected {
            background: rgba(255,255,255,0.2) !important;
        }
        """
        
        with gr.Blocks(css=custom_css, title="AIæç¤ºè¯æµ‹è¯•æ¡†æ¶") as interface:
            
            # æ ‡é¢˜å’Œè¯´æ˜
            gr.Markdown("""
            # ğŸ¯ AIæç¤ºè¯æµ‹è¯•æ¡†æ¶
            
            **åŸºäº DeepSeek R1 å…è´¹æ¨¡å‹çš„AIæµ‹è¯•å¹³å°**
            
            - ğŸ†“ **å®Œå…¨å…è´¹** - ä½¿ç”¨DeepSeek R1å…è´¹æ¨¡å‹ï¼Œæ— éœ€ä»˜è´¹
            - âš¡ **é«˜æ€§èƒ½æ¨ç†** - å¼ºå¤§çš„ä¸­æ–‡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›
            - ğŸ“Š **æ™ºèƒ½è¯„ä¼°** - å¤šç»´åº¦è´¨é‡è¯„ä¼°
            - ğŸ¯ **ä¸“ä¸šæµ‹è¯•** - é’ˆå¯¹æç¤ºè¯å·¥ç¨‹ä¼˜åŒ–
            """)
            
            # ä¸»è¦åŠŸèƒ½æ ‡ç­¾é¡µ
            with gr.Tabs():
                
                # å•æ¨¡å‹æµ‹è¯•
                with gr.TabItem("ğŸ¯ å•æ¨¡å‹æµ‹è¯•"):
                    with gr.Row():
                        with gr.Column(scale=1):
                            model_single = gr.Dropdown(
                                choices=self.get_model_choices(),
                                value=self.get_model_choices()[0] if self.get_model_choices() else "",
                                label="é€‰æ‹©æ¨¡å‹",
                                info="é€‰æ‹©è¦æµ‹è¯•çš„AIæ¨¡å‹"
                            )
                            
                            test_case_single = gr.Dropdown(
                                choices=self.get_test_case_choices(),
                                value=self.get_test_case_choices()[0] if self.get_test_case_choices() else "",
                                label="é€‰æ‹©æµ‹è¯•ç”¨ä¾‹",
                                info="é€‰æ‹©é¢„å®šä¹‰çš„æµ‹è¯•ç”¨ä¾‹"
                            )
                            
                            custom_prompt_single = gr.Textbox(
                                label="è‡ªå®šä¹‰æç¤ºè¯",
                                placeholder="è¾“å…¥è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¯é€‰ï¼Œä¼šè¦†ç›–æµ‹è¯•ç”¨ä¾‹ï¼‰",
                                lines=3
                            )
                            
                            run_single_btn = gr.Button("ğŸš€ è¿è¡Œæµ‹è¯•", variant="primary")
                        
                        with gr.Column(scale=2):
                            single_status = gr.Textbox(label="æµ‹è¯•çŠ¶æ€", interactive=False)
                            single_details = gr.Textbox(label="è¯¦ç»†ç»“æœ", lines=10, interactive=False)
                            single_response = gr.Textbox(label="AIå“åº”", lines=5, interactive=False)
                    
                    # æ¨¡å‹ä¿¡æ¯æ˜¾ç¤º
                    model_info_single = gr.Textbox(label="æ¨¡å‹ä¿¡æ¯", lines=5, interactive=False)
                    
                    # ç»‘å®šäº‹ä»¶
                    run_single_btn.click(
                        fn=self.run_single_test,
                        inputs=[model_single, test_case_single, custom_prompt_single],
                        outputs=[single_status, single_details, single_response]
                    )
                    
                    model_single.change(
                        fn=self.get_model_info,
                        inputs=[model_single],
                        outputs=[model_info_single]
                    )
                
                # æ¨¡å‹æ¯”è¾ƒ
                with gr.TabItem("ğŸ” æ¨¡å‹æ¯”è¾ƒ"):
                    with gr.Row():
                        with gr.Column(scale=1):
                            models_compare = gr.CheckboxGroup(
                                choices=self.get_model_choices(),
                                value=self.get_model_choices()[:2] if len(self.get_model_choices()) >= 2 else [],
                                label="é€‰æ‹©è¦æ¯”è¾ƒçš„æ¨¡å‹",
                                info="é€‰æ‹©2-5ä¸ªæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ"
                            )
                            
                            test_case_compare = gr.Dropdown(
                                choices=self.get_test_case_choices(),
                                value=self.get_test_case_choices()[0] if self.get_test_case_choices() else "",
                                label="é€‰æ‹©æµ‹è¯•ç”¨ä¾‹",
                                info="é€‰æ‹©é¢„å®šä¹‰çš„æµ‹è¯•ç”¨ä¾‹"
                            )
                            
                            custom_prompt_compare = gr.Textbox(
                                label="è‡ªå®šä¹‰æç¤ºè¯",
                                placeholder="è¾“å…¥è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¯é€‰ï¼Œä¼šè¦†ç›–æµ‹è¯•ç”¨ä¾‹ï¼‰",
                                lines=3
                            )
                            
                            run_compare_btn = gr.Button("ğŸ” å¼€å§‹æ¯”è¾ƒ", variant="primary")
                        
                        with gr.Column(scale=2):
                            compare_status = gr.Textbox(label="æ¯”è¾ƒçŠ¶æ€", interactive=False)
                            compare_results = gr.Textbox(label="æ¯”è¾ƒç»“æœ", lines=15, interactive=False)
                    
                    # ç»‘å®šäº‹ä»¶
                    run_compare_btn.click(
                        fn=self.run_model_comparison,
                        inputs=[models_compare, test_case_compare, custom_prompt_compare],
                        outputs=[compare_status, compare_results]
                    )
                
                # æ‰¹é‡æµ‹è¯•
                with gr.TabItem("ğŸ“¦ æ‰¹é‡æµ‹è¯•"):
                    with gr.Row():
                        with gr.Column(scale=1):
                            model_batch = gr.Dropdown(
                                choices=self.get_model_choices(),
                                value=self.get_model_choices()[0] if self.get_model_choices() else "",
                                label="é€‰æ‹©æ¨¡å‹",
                                info="é€‰æ‹©è¦è¿›è¡Œæ‰¹é‡æµ‹è¯•çš„æ¨¡å‹"
                            )
                            
                            test_suite = gr.Dropdown(
                                choices=["basic", "comprehensive", "ssap", "workflow"],
                                value="basic",
                                label="æµ‹è¯•å¥—ä»¶",
                                info="é€‰æ‹©æµ‹è¯•å¥—ä»¶ç±»å‹"
                            )
                            
                            run_batch_btn = gr.Button("ğŸ“¦ è¿è¡Œæ‰¹é‡æµ‹è¯•", variant="primary")
                        
                        with gr.Column(scale=2):
                            batch_status = gr.Textbox(label="æµ‹è¯•çŠ¶æ€", interactive=False)
                            batch_results = gr.Textbox(label="æ‰¹é‡æµ‹è¯•ç»“æœ", lines=15, interactive=False)
                    
                    # ç»‘å®šäº‹ä»¶
                    run_batch_btn.click(
                        fn=self.run_batch_test,
                        inputs=[model_batch, test_suite],
                        outputs=[batch_status, batch_results]
                    )
                
                # ç³»ç»Ÿè®¾ç½®
                with gr.TabItem("âš™ï¸ ç³»ç»Ÿè®¾ç½®"):
                    gr.Markdown("""
                    ### ç³»ç»ŸçŠ¶æ€
                    """)
                    
                    with gr.Row():
                        with gr.Column():
                            if self.client:
                                gr.Markdown("âœ… **OpenRouterå®¢æˆ·ç«¯:** å·²è¿æ¥")
                            else:
                                gr.Markdown("âŒ **OpenRouterå®¢æˆ·ç«¯:** æœªè¿æ¥")
                            
                            gr.Markdown(f"ğŸ“ **æµ‹è¯•ç”¨ä¾‹æ•°é‡:** {len(self.test_cases)}")
                            gr.Markdown(f"ğŸ¤– **å¯ç”¨æ¨¡å‹æ•°é‡:** {len(self.available_models)}")
                    
                    gr.Markdown("""
                    ### é…ç½®è¯´æ˜
                    
                    1. **APIé…ç½®**: ä¿®æ”¹ `config/openrouter_config.json` æ–‡ä»¶
                    2. **æµ‹è¯•ç”¨ä¾‹**: ç¼–è¾‘ `examples/example_test_cases.json` æ–‡ä»¶
                    3. **é‡æ–°åŠ è½½**: é‡å¯ç•Œé¢ä»¥åº”ç”¨æ–°é…ç½®
                    
                    ### æ”¯æŒçš„æ¨¡å‹
                    
                    - **DeepSeek R1 0528** - å…è´¹é«˜æ€§èƒ½æ¨ç†æ¨¡å‹
                    - **Claude 3.5 Sonnet** - å¤æ‚æ¨ç†ä»»åŠ¡
                    - **Claude 3 Haiku** - å¿«é€Ÿå“åº”
                    - **GPT-4 Turbo** - åˆ›æ„å†™ä½œ
                    - **GPT-3.5 Turbo** - é€šç”¨ä»»åŠ¡
                    - **Llama 3.1 405B** - å¼€æºç ”ç©¶
                    
                    ### æˆæœ¬ç®¡ç†
                    
                    - å®æ—¶æˆæœ¬è¿½è¸ª
                    - é¢„ç®—æ§åˆ¶è®¾ç½®
                    - å…è´¹æ¨¡å‹ä¼˜å…ˆæ¨è
                    """)
            
            # åº•éƒ¨ä¿¡æ¯
            gr.Markdown("""
            ---
            **AIæç¤ºè¯æµ‹è¯•æ¡†æ¶** | åŸºäºOpenRouter API | æ”¯æŒå¤šç§AIæ¨¡å‹ | å®Œå…¨å…è´¹çš„DeepSeek R1æ¨¡å‹
            """)
        
        return interface

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨AIæç¤ºè¯æµ‹è¯•æ¡†æ¶Webç•Œé¢...")
    
    # åˆ›å»ºç•Œé¢å®ä¾‹
    interface_manager = GradioTestInterface()
    
    # åˆ›å»ºGradioç•Œé¢
    interface = interface_manager.create_interface()
    
    # å¯åŠ¨ç•Œé¢
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=False,
        show_error=True,
        quiet=False
    )

if __name__ == "__main__":
    main()