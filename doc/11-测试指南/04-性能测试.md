# 性能测试指南

## 🎯 性能测试概述

性能测试确保OPUS系统在各种负载条件下的响应时间、吞吐量和资源使用满足业务需求。

## 📊 性能指标

### 关键性能指标 (KPI)
- **响应时间** - 智能体生成 < 30秒
- **吞吐量** - 并发用户 ≥ 100
- **资源使用** - 内存 < 512MB，CPU < 80%
- **可用性** - 99.9% 正常运行时间

### 具体性能目标
```yaml
性能基准:
  智能体生成:
    平均响应时间: < 15秒
    P95响应时间: < 30秒
    P99响应时间: < 60秒
  
  语法解析:
    平均响应时间: < 50ms
    P95响应时间: < 100ms
    P99响应时间: < 200ms
  
  并发处理:
    最大并发用户: 100
    平均吞吐量: 10 req/sec
    峰值吞吐量: 50 req/sec
```

## 🛠️ 性能测试工具

### Python性能测试框架
```python
# requirements-performance.txt
pytest-benchmark>=4.0.0
locust>=2.14.0
memory-profiler>=0.60.0
line-profiler>=4.0.0
pytest-monitor>=1.6.0
psutil>=5.9.0
```

### 测试工具配置
```python
# pytest-benchmark配置
pytest.ini
[tool:pytest]
addopts = --benchmark-only --benchmark-sort=mean
markers =
    performance: marks tests as performance tests
    benchmark: marks tests for benchmarking
    stress: marks tests as stress tests
```

## 📋 性能测试类型

### 基准测试 (Benchmark)
```python
import pytest
from opus.core import AgentGenerator

class TestPerformanceBenchmarks:
    """性能基准测试"""
    
    @pytest.mark.benchmark
    def test_agent_generation_benchmark(self, benchmark):
        """智能体生成性能基准"""
        generator = AgentGenerator()
        request = {
            "description": "Python代码审查助手",
            "requirements": ["代码分析", "建议生成"]
        }
        
        # 执行基准测试
        result = benchmark(generator.generate, request)
        
        # 验证结果
        assert result.success
        assert result.agent.is_valid()
    
    @pytest.mark.benchmark
    def test_opus_parsing_benchmark(self, benchmark):
        """OPUS语法解析性能基准"""
        from opus.parser import OpusParser
        
        opus_content = """
        # Identity
        name: 复杂助手
        role: 高级AI助手
        expertise: [AI, ML, NLP, Python, JavaScript]
        
        # Architecture
        knowledge_domains: [技术, 科学, 商业]
        skills: [分析, 生成, 推理, 创作]
        
        # Memory
        type: long_term
        retention: 90d
        structure: hierarchical
        
        # Workflow
        steps:
          - input_analysis
          - context_retrieval
          - reasoning
          - response_generation
          - quality_check
        """
        
        parser = OpusParser()
        result = benchmark(parser.parse, opus_content)
        
        assert result.is_valid()
```

### 负载测试 (Load Testing)
```python
from locust import HttpUser, task, between

class OpusLoadTest(HttpUser):
    """OPUS系统负载测试"""
    
    wait_time = between(1, 5)  # 用户请求间隔
    
    def on_start(self):
        """测试开始时的初始化"""
        self.client.verify = False  # 忽略SSL验证
    
    @task(3)
    def generate_simple_agent(self):
        """生成简单智能体（高频任务）"""
        response = self.client.post("/api/generate", json={
            "description": "简单的问答助手",
            "type": "basic"
        })
        
        if response.status_code == 200:
            assert "agent" in response.json()
    
    @task(2)
    def generate_complex_agent(self):
        """生成复杂智能体（中频任务）"""
        response = self.client.post("/api/generate", json={
            "description": "专业的代码审查助手",
            "requirements": [
                "多语言支持",
                "性能分析",
                "安全检查"
            ],
            "type": "advanced"
        })
        
        if response.status_code == 200:
            result = response.json()
            assert result["agent"]["complexity"] == "advanced"
    
    @task(1)
    def analyze_requirements(self):
        """需求分析（低频任务）"""
        response = self.client.post("/api/analyze", json={
            "description": "我需要一个能够处理客户服务的AI助手"
        })
        
        assert response.status_code == 200
```

### 压力测试 (Stress Testing)
```python
import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor

class StressTest:
    """系统压力测试"""
    
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
    
    async def single_request(self, session, request_data):
        """单个请求"""
        try:
            start_time = time.time()
            async with session.post(
                f"{self.base_url}/api/generate",
                json=request_data
            ) as response:
                result = await response.json()
                end_time = time.time()
                
                return {
                    "success": response.status == 200,
                    "response_time": end_time - start_time,
                    "result": result
                }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response_time": None
            }
    
    async def stress_test(self, concurrent_users=50, duration=300):
        """执行压力测试"""
        print(f"开始压力测试: {concurrent_users}并发用户, {duration}秒")
        
        start_time = time.time()
        results = []
        
        async with aiohttp.ClientSession() as session:
            while time.time() - start_time < duration:
                # 创建并发请求
                tasks = []
                for i in range(concurrent_users):
                    request_data = {
                        "description": f"压力测试助手{i}",
                        "type": "basic"
                    }
                    tasks.append(self.single_request(session, request_data))
                
                # 执行并发请求
                batch_results = await asyncio.gather(*tasks)
                results.extend(batch_results)
                
                # 短暂休息
                await asyncio.sleep(1)
        
        return self.analyze_results(results)
    
    def analyze_results(self, results):
        """分析测试结果"""
        successful = [r for r in results if r["success"]]
        failed = [r for r in results if not r["success"]]
        
        response_times = [r["response_time"] for r in successful if r["response_time"]]
        
        return {
            "total_requests": len(results),
            "successful_requests": len(successful),
            "failed_requests": len(failed),
            "success_rate": len(successful) / len(results) * 100,
            "avg_response_time": sum(response_times) / len(response_times) if response_times else 0,
            "max_response_time": max(response_times) if response_times else 0,
            "min_response_time": min(response_times) if response_times else 0
        }

# 运行压力测试
async def run_stress_test():
    stress_test = StressTest()
    results = await stress_test.stress_test(concurrent_users=20, duration=60)
    print(f"压力测试结果: {results}")

# pytest使用
@pytest.mark.stress
@pytest.mark.asyncio
async def test_system_stress():
    """系统压力测试"""
    stress_test = StressTest()
    results = await stress_test.stress_test(concurrent_users=10, duration=30)
    
    assert results["success_rate"] >= 95  # 成功率要求
    assert results["avg_response_time"] <= 30  # 平均响应时间要求
```

## 📈 性能监控

### 资源使用监控
```python
import psutil
import time
import threading
from dataclasses import dataclass

@dataclass
class SystemMetrics:
    """系统指标"""
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    disk_io_read_mb: float
    disk_io_write_mb: float
    network_sent_mb: float
    network_recv_mb: float
    timestamp: float

class PerformanceMonitor:
    """性能监控器"""
    
    def __init__(self):
        self.metrics = []
        self.monitoring = False
        self.monitor_thread = None
    
    def start_monitoring(self, interval=1):
        """开始监控"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            args=(interval,)
        )
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """停止监控"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
    
    def _monitor_loop(self, interval):
        """监控循环"""
        while self.monitoring:
            metrics = self._collect_metrics()
            self.metrics.append(metrics)
            time.sleep(interval)
    
    def _collect_metrics(self):
        """收集系统指标"""
        return SystemMetrics(
            cpu_percent=psutil.cpu_percent(),
            memory_percent=psutil.virtual_memory().percent,
            memory_used_mb=psutil.virtual_memory().used / 1024 / 1024,
            disk_io_read_mb=psutil.disk_io_counters().read_bytes / 1024 / 1024,
            disk_io_write_mb=psutil.disk_io_counters().write_bytes / 1024 / 1024,
            network_sent_mb=psutil.net_io_counters().bytes_sent / 1024 / 1024,
            network_recv_mb=psutil.net_io_counters().bytes_recv / 1024 / 1024,
            timestamp=time.time()
        )
    
    def get_summary(self):
        """获取监控摘要"""
        if not self.metrics:
            return None
        
        cpu_values = [m.cpu_percent for m in self.metrics]
        memory_values = [m.memory_percent for m in self.metrics]
        
        return {
            "duration": self.metrics[-1].timestamp - self.metrics[0].timestamp,
            "avg_cpu_percent": sum(cpu_values) / len(cpu_values),
            "max_cpu_percent": max(cpu_values),
            "avg_memory_percent": sum(memory_values) / len(memory_values),
            "max_memory_percent": max(memory_values),
            "peak_memory_mb": max(m.memory_used_mb for m in self.metrics)
        }

# 使用示例
@pytest.mark.performance
def test_agent_generation_with_monitoring():
    """带监控的智能体生成测试"""
    monitor = PerformanceMonitor()
    
    try:
        # 开始监控
        monitor.start_monitoring(interval=0.5)
        
        # 执行测试
        generator = AgentGenerator()
        for i in range(10):
            result = generator.generate({
                "description": f"测试助手{i}",
                "type": "basic"
            })
            assert result.success
        
        # 停止监控
        monitor.stop_monitoring()
        
        # 分析结果
        summary = monitor.get_summary()
        print(f"性能摘要: {summary}")
        
        # 断言性能要求
        assert summary["max_cpu_percent"] < 80  # CPU使用率不超过80%
        assert summary["peak_memory_mb"] < 512  # 内存使用不超过512MB
        
    finally:
        monitor.stop_monitoring()
```

## 🔍 性能分析

### 代码性能分析
```python
# 使用line_profiler进行行级性能分析
@profile
def generate_agent_profiled(description):
    """带性能分析的智能体生成"""
    generator = AgentGenerator()
    
    # 步骤1: 需求分析
    analysis = generator.analyze_requirements(description)
    
    # 步骤2: 配置生成
    config = generator.generate_config(analysis)
    
    # 步骤3: 智能体创建
    agent = generator.create_agent(config)
    
    return agent

# 使用memory_profiler进行内存分析
@profile
def memory_intensive_operation():
    """内存密集型操作分析"""
    large_data = []
    for i in range(10000):
        large_data.append({
            "id": i,
            "data": "x" * 1000,
            "metadata": {"created": time.time()}
        })
    
    # 处理数据
    processed = [item for item in large_data if item["id"] % 2 == 0]
    
    return processed
```

### 性能瓶颈识别
```python
import cProfile
import pstats
from contextlib import contextmanager

@contextmanager
def profile_code():
    """代码性能分析上下文管理器"""
    profiler = cProfile.Profile()
    profiler.enable()
    
    try:
        yield profiler
    finally:
        profiler.disable()

def analyze_performance_bottlenecks():
    """分析性能瓶颈"""
    with profile_code() as profiler:
        # 执行被分析的代码
        generator = AgentGenerator()
        for i in range(5):
            generator.generate({
                "description": f"性能测试助手{i}",
                "complexity": "high"
            })
    
    # 分析结果
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    
    # 输出top 10耗时函数
    print("Top 10 most time-consuming functions:")
    stats.print_stats(10)
    
    return stats
```

## 📊 性能测试报告

### 自动化报告生成
```python
import json
import matplotlib.pyplot as plt
from datetime import datetime

class PerformanceReporter:
    """性能测试报告生成器"""
    
    def __init__(self):
        self.results = []
    
    def add_test_result(self, test_name, metrics):
        """添加测试结果"""
        self.results.append({
            "test_name": test_name,
            "timestamp": datetime.now().isoformat(),
            "metrics": metrics
        })
    
    def generate_report(self, output_path="performance_report.html"):
        """生成HTML性能报告"""
        html_content = self._create_html_report()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"性能报告已生成: {output_path}")
    
    def _create_html_report(self):
        """创建HTML报告内容"""
        return f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>OPUS性能测试报告</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .metric {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; }}
                .success {{ background-color: #d4edda; }}
                .warning {{ background-color: #fff3cd; }}
                .danger {{ background-color: #f8d7da; }}
            </style>
        </head>
        <body>
            <h1>OPUS性能测试报告</h1>
            <p>生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            
            <h2>测试摘要</h2>
            {self._generate_summary_html()}
            
            <h2>详细结果</h2>
            {self._generate_details_html()}
        </body>
        </html>
        """
    
    def _generate_summary_html(self):
        """生成摘要HTML"""
        if not self.results:
            return "<p>暂无测试结果</p>"
        
        total_tests = len(self.results)
        avg_response_time = sum(
            r["metrics"].get("avg_response_time", 0) 
            for r in self.results
        ) / total_tests
        
        return f"""
        <div class="metric success">
            <h3>总体指标</h3>
            <p>测试数量: {total_tests}</p>
            <p>平均响应时间: {avg_response_time:.2f}秒</p>
        </div>
        """
    
    def _generate_details_html(self):
        """生成详细结果HTML"""
        details_html = ""
        
        for result in self.results:
            css_class = self._get_result_css_class(result["metrics"])
            details_html += f"""
            <div class="metric {css_class}">
                <h3>{result['test_name']}</h3>
                <p>执行时间: {result['timestamp']}</p>
                {self._format_metrics_html(result['metrics'])}
            </div>
            """
        
        return details_html
    
    def _get_result_css_class(self, metrics):
        """根据指标确定CSS类"""
        response_time = metrics.get("avg_response_time", 0)
        success_rate = metrics.get("success_rate", 100)
        
        if response_time > 30 or success_rate < 95:
            return "danger"
        elif response_time > 15 or success_rate < 98:
            return "warning"
        else:
            return "success"
    
    def _format_metrics_html(self, metrics):
        """格式化指标为HTML"""
        html = "<ul>"
        for key, value in metrics.items():
            html += f"<li>{key}: {value}</li>"
        html += "</ul>"
        return html

# 使用示例
def test_with_reporting():
    """带报告的性能测试"""
    reporter = PerformanceReporter()
    
    # 执行多个性能测试
    tests = [
        ("智能体生成", {"avg_response_time": 12.5, "success_rate": 99.2}),
        ("语法解析", {"avg_response_time": 0.08, "success_rate": 100.0}),
        ("并发处理", {"avg_response_time": 18.3, "success_rate": 97.8})
    ]
    
    for test_name, metrics in tests:
        reporter.add_test_result(test_name, metrics)
    
    # 生成报告
    reporter.generate_report()
```

## 🚦 持续性能监控

### CI/CD性能监控集成
```yaml
# .github/workflows/performance.yml
name: Performance Tests

on:
  push:
    branches: [main]
  schedule:
    - cron: '0 2 * * *'  # 每天凌晨2点运行

jobs:
  performance:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements-performance.txt
    
    - name: Run performance tests
      run: |
        pytest tests/performance/ --benchmark-json=benchmark.json
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: benchmark.json
    
    - name: Performance regression check
      run: |
        python scripts/check_performance_regression.py benchmark.json
```

## 🎓 最佳实践

### 性能测试策略
1. **早期集成** - 在开发过程中持续进行性能测试
2. **真实数据** - 使用接近生产环境的数据量
3. **多维度测试** - 响应时间、吞吐量、资源使用
4. **回归检测** - 监控性能退化

### 常见误区
- ❌ **只测试最佳情况** - 忽略边界和异常情况
- ❌ **测试环境不真实** - 与生产环境差异过大
- ❌ **忽略资源监控** - 只关注响应时间
- ❌ **缺乏基准对比** - 没有历史性能基准

## 📚 相关文档

- [测试概述](01-测试概述.md) - 整体测试策略
- [单元测试指南](02-单元测试.md) - 单元测试方法
- [集成测试指南](03-集成测试.md) - 集成测试策略
- [自动化测试](06-自动化测试.md) - CI/CD集成

---

*🎯 性能测试确保系统在各种负载下稳定运行，是产品上线的重要保障。*